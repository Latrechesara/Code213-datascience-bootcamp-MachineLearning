{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align:left;\">\n",
        "  <a href=\"https://code213.tech/\" target=\"_blank\">\n",
        "    <img src=\"../images/code213.PNG\" alt=\"QWorld\">\n",
        "  </a>\n",
        "  <p><em>prepared by Latreche Sara</em></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t16BX732VfXh"
      },
      "source": [
        "# Cluster the Olivetti Faces Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5kH3N62mxJf"
      },
      "source": [
        "## Load the Olivetti Faces Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUDZoMIPVU7G"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import the dataset loading function from scikit-learn\n",
        "# Hint: Use the function `fetch_olivetti_faces()` from the module `sklearn.datasets`.\n",
        "\n",
        "#  Step 2: Load the Olivetti faces dataset\n",
        "# Hint: Call the function `fetch_olivetti_faces()` and assign it to a variable named `olivetti`.\n",
        "# This function returns a dictionary-like object with face data, labels, images, and a description.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEUW0cULlECS",
        "outputId": "2f884afa-9540-43ad-c3b2-547357c31a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to /root/scikit_learn_data\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "\n",
        "olivetti= fetch_olivetti_faces()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vU6mOfVk1UD"
      },
      "outputs": [],
      "source": [
        "# 📄 Step 3: Display the dataset description\n",
        "# Hint: The 'olivetti' object has an attribute called 'DESCR' that contains a description of the dataset.\n",
        "# This will help you understand the background and structure of the data you're working with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvbUci_fmMKd",
        "outputId": "da41d40d-d43c-4ae6-b224-fae1bb4a4bee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".. _olivetti_faces_dataset:\n",
            "\n",
            "The Olivetti faces dataset\n",
            "--------------------------\n",
            "\n",
            "`This dataset contains a set of face images`_ taken between April 1992 and\n",
            "April 1994 at AT&T Laboratories Cambridge. The\n",
            ":func:`sklearn.datasets.fetch_olivetti_faces` function is the data\n",
            "fetching / caching function that downloads the data\n",
            "archive from AT&T.\n",
            "\n",
            ".. _This dataset contains a set of face images: https://cam-orl.co.uk/facedatabase.html\n",
            "\n",
            "As described on the original website:\n",
            "\n",
            "    There are ten different images of each of 40 distinct subjects. For some\n",
            "    subjects, the images were taken at different times, varying the lighting,\n",
            "    facial expressions (open / closed eyes, smiling / not smiling) and facial\n",
            "    details (glasses / no glasses). All the images were taken against a dark\n",
            "    homogeneous background with the subjects in an upright, frontal position\n",
            "    (with tolerance for some side movement).\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "=================   =====================\n",
            "Classes                                40\n",
            "Samples total                         400\n",
            "Dimensionality                       4096\n",
            "Features            real, between 0 and 1\n",
            "=================   =====================\n",
            "\n",
            "The image is quantized to 256 grey levels and stored as unsigned 8-bit\n",
            "integers; the loader will convert these to floating point values on the\n",
            "interval [0, 1], which are easier to work with for many algorithms.\n",
            "\n",
            "The \"target\" for this database is an integer from 0 to 39 indicating the\n",
            "identity of the person pictured; however, with only 10 examples per class, this\n",
            "relatively small dataset is more interesting from an unsupervised or\n",
            "semi-supervised perspective.\n",
            "\n",
            "The original dataset consisted of 92 x 112, while the version available here\n",
            "consists of 64x64 images.\n",
            "\n",
            "When using these images, please give credit to AT&T Laboratories Cambridge.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(olivetti.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIemvnRaozCn"
      },
      "outputs": [],
      "source": [
        "# Step 3: Access the target labels of the dataset\n",
        "# Hint: Use the `.target` attribute of the `olivetti` object to get the labels.\n",
        "# These labels are integers from 0 to 39, identifying the person in each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5w18Cubo01K",
        "outputId": "20818736-9460-47cf-9597-94c55de93fb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
              "        3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,\n",
              "        5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,\n",
              "        6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,\n",
              "        8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
              "       10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
              "       11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
              "       13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15,\n",
              "       15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
              "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,\n",
              "       18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20,\n",
              "       20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22,\n",
              "       22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23,\n",
              "       23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25,\n",
              "       25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27,\n",
              "       27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
              "       28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30,\n",
              "       30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32,\n",
              "       32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
              "       34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35,\n",
              "       35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37,\n",
              "       37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39,\n",
              "       39, 39, 39, 39, 39, 39, 39, 39, 39])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "olivetti.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hanH-lY8pm6A"
      },
      "source": [
        "### What is Stratified Sampling?\n",
        "Stratified sampling is a technique used when splitting a dataset into subsets (like training, validation, and test sets) to preserve the distribution of the target labels.\n",
        "\n",
        "### Why do we use it?\n",
        "In classification problems—especially with imbalanced or small datasets like Olivetti Faces—you want to ensure that each class (person) is represented equally in every subset of your data (training, validation, test).\n",
        "Without it, some classes might appear more in one split than others, which can lead to biased model performance.\n",
        "\n",
        "**This ensures** that the proportion of images for each of the 40 people is approximately the same in the training, validation, and test sets.\n",
        "\n",
        "Example:\n",
        "If person 0 appears 10 times in the dataset:\n",
        "\n",
        "With stratified sampling, person 0 will appear 8 times in the 80% training+validation set and 2 times in the 20% test set.\n",
        "\n",
        "Without it, you could randomly end up with, say, all of person 0’s images in just the training set—and none in validation or test—which is bad for generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erm3Epqno4Af"
      },
      "outputs": [],
      "source": [
        "# Step 5: Use stratified sampling to split the dataset manually\n",
        "# You will use `StratifiedShuffleSplit` from `sklearn.model_selection` to ensure\n",
        "# each class (person) is equally represented in all splits.\n",
        "\n",
        "\n",
        "# First split: Create test set with 40 samples (1 image per person)\n",
        "# Method used: StratifiedShuffleSplit(n_splits=1, test_size=40)\n",
        "# Variable: strat_split defines the split object\n",
        "\n",
        "# Use .split(X, y) to generate indices for train/validation and test sets\n",
        "# Variables: train_valid_idx and test_idx\n",
        "\n",
        "# Use the indices to extract the corresponding data and labels\n",
        "\n",
        "#  Second split: Split train_valid into 80 training and 80 validation samples\n",
        "# Method used: StratifiedShuffleSplit(n_splits=1, test_size=80)\n",
        "# This ensures each person still has 2 images in both training and validation sets.\n",
        "\n",
        "# Split training/validation data using the new stratified splitter\n",
        "\n",
        "\n",
        "\n",
        "# Create the final training and validation sets\n",
        "X_train =\n",
        "y_train =\n",
        "X_valid =\n",
        "y_valid =\n",
        "\n",
        "# Output the number of samples in each set\n",
        "\n",
        "print(\"Training set size\":,)\n",
        "print(\"Validation set size\":,)\n",
        "print(\"Test set size\":,)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI-gRhhnrV5V",
        "outputId": "afd3e795-3883-482b-de5a-7e6646a80248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: (280, 4096)\n",
            "Validation set size: (80, 4096)\n",
            "Test set size: (40, 4096)\n"
          ]
        }
      ],
      "source": [
        "#  Step 5: Use stratified sampling to split the dataset manually\n",
        "# You will use `StratifiedShuffleSplit` from `sklearn.model_selection` to ensure\n",
        "# each class (person) is equally represented in all splits.\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "#  First split: Create test set with 40 samples (1 image per person)\n",
        "# Method used: StratifiedShuffleSplit(n_splits=1, test_size=40)\n",
        "# Variable: strat_split defines the split object\n",
        "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=40, random_state=42)\n",
        "\n",
        "# Use .split(X, y) to generate indices for train/validation and test sets\n",
        "# Variables: train_valid_idx and test_idx\n",
        "train_valid_idx, test_idx = next(strat_split.split(olivetti.data, olivetti.target))\n",
        "\n",
        "# Use the indices to extract the corresponding data and labels\n",
        "X_train_valid = olivetti.data[train_valid_idx]\n",
        "y_train_valid = olivetti.target[train_valid_idx]\n",
        "X_test = olivetti.data[test_idx]\n",
        "y_test = olivetti.target[test_idx]\n",
        "\n",
        "#  Second split: Split train_valid into 80 training and 80 validation samples\n",
        "# Method used: StratifiedShuffleSplit(n_splits=1, test_size=80)\n",
        "# This ensures each person still has 2 images in both training and validation sets.\n",
        "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=80, random_state=43)\n",
        "\n",
        "# Split training/validation data using the new stratified splitter\n",
        "train_idx, valid_idx = next(strat_split.split(X_train_valid, y_train_valid))\n",
        "\n",
        "# Create the final training and validation sets\n",
        "X_train = X_train_valid[train_idx]\n",
        "y_train = y_train_valid[train_idx]\n",
        "X_valid = X_train_valid[valid_idx]\n",
        "y_valid = y_train_valid[valid_idx]\n",
        "\n",
        "# 🧾 Output the number of samples in each set\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Validation set size:\", X_valid.shape)\n",
        "print(\"Test set size:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mO_OMrr97jF"
      },
      "source": [
        "###  Dataset Split Summary\n",
        "\n",
        "After applying **Stratified Sampling**, the Olivetti Faces dataset is divided into three sets:\n",
        "\n",
        "| Dataset        | Number of Samples | Shape (samples × features) | Description |\n",
        "|----------------|-------------------|-----------------------------|-------------|\n",
        "| **Training**   | 280               | (280, 4096)                 | Used to train the model |\n",
        "| **Validation** | 80                | (80, 4096)                  | Used to tune model parameters and prevent overfitting |\n",
        "| **Test**       | 40                | (40, 4096)                  | Used to evaluate the final model performance |\n",
        "\n",
        "- Each image is originally **64 × 64 pixels**, flattened into a vector of **4096** pixel values (features).\n",
        "- There are a total of **400 images**, evenly representing **40 individuals (10 images per person)**.\n",
        "- We use **stratified sampling** to maintain equal class distribution across all splits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA41w-uH_YT_"
      },
      "source": [
        "# PCA for data dimentionality\n",
        "\n",
        "To speed things up, we'll reduce the data's dimensionality using PCA:\n",
        "\n",
        "###  Dimensionality Reduction with PCA\n",
        "\n",
        "To speed things up and simplify our analysis, we'll use **Principal Component Analysis (PCA)** to reduce the dataset's dimensionality.\n",
        "\n",
        "- Each face image currently has **4096 features** (one for each pixel).\n",
        "- This high dimensionality can be computationally expensive and may include redundant information.\n",
        "- PCA helps us:\n",
        "  -  **Compress** the data into fewer dimensions,\n",
        "  -  **Capture the most important variance** in the dataset,\n",
        "  -  **Improve clustering performance and speed**.\n",
        "\n",
        "We’ll use **`sklearn.decomposition.PCA`** to reduce the data to a smaller number of principal components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzGnu6AWBrYh"
      },
      "source": [
        "-  What Does `PCA(n_components=0.99)` Mean?\n",
        "\n",
        "Instead of manually choosing the number of dimensions to reduce to, we ask PCA to:\n",
        "> 💬 \"Keep enough components to preserve **99% of the variance** (information) in the original data.\"\n",
        "- Why Transform X_valid and X_test?\n",
        "After training PCA on the training set (X_train), we apply the same transformation to:\n",
        "_Validation set (X_valid)\n",
        "_Test set (X_test)\n",
        "This ensures that all datasets are projected into the same reduced feature space, preventing data leakage and keeping the evaluation fair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSw9kLPBAMug"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import the PCA class from sklearn.decomposition\n",
        "# Method: PCA\n",
        "\n",
        "# Step 2: Create a PCA object to preserve 99% of the variance\n",
        "# Variable to use: pca\n",
        "# Constructor method: PCA(n_components=0.99)\n",
        "\n",
        "# Step 3: Fit PCA to the training set and transform it\n",
        "# Method: fit_transform\n",
        "# Variable: X_train_pca\n",
        "\n",
        "# Step 4: Use the trained PCA to transform the validation set\n",
        "# Method: transform\n",
        "# Variable: X_valid_pca\n",
        "\n",
        "# Step 5: Use the same PCA transformation on the test set\n",
        "# Method: transform\n",
        "# Variable: X_test_pca\n",
        "\n",
        "# Step 6: Display the number of principal components chosen to preserve 99% of the variance\n",
        "# Attribute: n_components_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRBZWkcy99Et",
        "outputId": "701a21c4-1f32-45e2-e2ff-88d7c4bfb55b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.int64(199)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 1: Import the PCA class from sklearn.decomposition\n",
        "# Method: PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 2: Create a PCA object to preserve 99% of the variance\n",
        "# Variable to use: pca\n",
        "# Constructor method: PCA(n_components=0.99)\n",
        "pca = PCA(n_components=0.99)\n",
        "\n",
        "# Step 3: Fit PCA to the training set and transform it\n",
        "# Method: fit_transform\n",
        "# Variable: X_train_pca\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "\n",
        "# Step 4: Use the trained PCA to transform the validation set\n",
        "# Method: transform\n",
        "# Variable: X_valid_pca\n",
        "X_valid_pca = pca.transform(X_valid)\n",
        "\n",
        "# Step 5: Use the same PCA transformation on the test set\n",
        "# Method: transform\n",
        "# Variable: X_test_pca\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Step 6: Display the number of principal components chosen to preserve 99% of the variance\n",
        "# Attribute: n_components_\n",
        "pca.n_components_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPgtKui6COr1"
      },
      "source": [
        "# K-means clustering\n",
        "Cluster the images using K-Means, and ensure that you have a good number of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P60y--u_Djxb"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FUf5rFuC_sy"
      },
      "outputs": [],
      "source": [
        "#  Step: Perform KMeans Clustering on PCA-Transformed Data\n",
        "# -----------------------------------------------------------\n",
        "# In this step, we will apply KMeans clustering to the Olivetti dataset that has been transformed using PCA.\n",
        "# We'll test different values for 'k' (number of clusters) ranging from 5 to 145, in steps of 5.\n",
        "#\n",
        "#  Method: sklearn.cluster.KMeans\n",
        "#  Variables to use:\n",
        "#     - X_train_pca: your PCA-transformed training set\n",
        "#     - kmeans_per_k: a list to store the resulting KMeans models for each 'k'\n",
        "#\n",
        "#  Your task:\n",
        "# Loop through a range of 'k' values, create a KMeans model for each, fit it to X_train_pca, and store the fitted model in the list.\n",
        "\n",
        "\n",
        " # Try cluster counts from 5 to 145 (step = 5)\n",
        "  # List to store fitted KMeans models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvibn5KgC5Gb"
      },
      "outputs": [],
      "source": [
        "#  Step: Perform KMeans Clustering on PCA-Transformed Data\n",
        "# -----------------------------------------------------------\n",
        "# In this step, we will apply KMeans clustering to the Olivetti dataset that has been transformed using PCA.\n",
        "# We'll test different values for 'k' (number of clusters) ranging from 5 to 145, in steps of 5.\n",
        "#\n",
        "#  Method: sklearn.cluster.KMeans\n",
        "#  Variables to use:\n",
        "#     - X_train_pca: your PCA-transformed training set\n",
        "#     - kmeans_per_k: a list to store the resulting KMeans models for each 'k'\n",
        "#\n",
        "#  Your task:\n",
        "# Loop through a range of 'k' values, create a KMeans model for each, fit it to X_train_pca, and store the fitted model in the list.\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k_range = range(5, 150, 5)  # Try cluster counts from 5 to 145 (step = 5)\n",
        "kmeans_per_k = []  # List to store fitted KMeans models\n",
        "\n",
        "for k in k_range:\n",
        "    print(f\"k = {k}\")\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_train_pca)\n",
        "    kmeans_per_k.append(kmeans)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znqxfBZCDpo0"
      },
      "source": [
        "## Evaluating the model and fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmaH4uizCEfS"
      },
      "outputs": [],
      "source": [
        "\n",
        "#  Step: Evaluate KMeans Clustering Using Silhouette Score\n",
        "# ----------------------------------------------------------\n",
        "# Now that we have fitted KMeans models for different cluster counts (k),\n",
        "# we will evaluate each model using the Silhouette score, which measures\n",
        "# how well-separated the clusters are.\n",
        "#\n",
        "#  Methods and functions to use:\n",
        "#   - sklearn.metrics.silhouette_score(X, labels)\n",
        "#   - np.argmax() to find the best silhouette score index\n",
        "#   - matplotlib.pyplot for plotting\n",
        "#\n",
        "#  Variables:\n",
        "#   - X_train_pca: PCA-transformed training data\n",
        "#   - kmeans_per_k: list of fitted KMeans models\n",
        "#   - k_range: list/range of k values tried\n",
        "#\n",
        "#  Your task:\n",
        "# 1. Calculate silhouette scores for each model.\n",
        "# 2. Find the best k with the highest silhouette score.\n",
        "# 3. Plot the silhouette scores vs k and highlight the best k.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwafHd3WDcFq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "silhouette_scores = [silhouette_score(X_train_pca, model.labels_) for model in kmeans_per_k]\n",
        "\n",
        "best_index = np.argmax(silhouette_scores)    # Index of highest silhouette score\n",
        "best_k = k_range[best_index]                  # Corresponding best number of clusters\n",
        "best_score = silhouette_scores[best_index]   # Best silhouette score value\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(k_range, silhouette_scores, \"bo-\", label=\"Silhouette Score\")\n",
        "plt.xlabel(\"$k$\", fontsize=14)\n",
        "plt.ylabel(\"Silhouette Score\", fontsize=14)\n",
        "plt.plot(best_k, best_score, \"rs\", label=f\"Best k = {best_k}\")\n",
        "plt.legend()\n",
        "plt.title(\"Silhouette Scores for Different k Values\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om9uLWKDD0vi"
      },
      "outputs": [],
      "source": [
        "best_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gwM2NHzE2GB"
      },
      "source": [
        "###  Why Is the Best Number of Clusters Much Higher Than 40?\n",
        "\n",
        "You might expect the best number of clusters to be **40**, because there are exactly 40 different people in the dataset. However, the clustering algorithm suggests a much higher number (e.g., 120).\n",
        "\n",
        "#### Why does this happen?\n",
        "\n",
        "- **Variability within each person’s images:** The same person can look quite different in different photos. For example:\n",
        "  - Wearing glasses in some images but not others\n",
        "  - Different facial expressions or head poses\n",
        "  - Lighting or shadows shifting the appearance\n",
        "  - Slight shifts in alignment or cropping\n",
        "\n",
        "- Because of these differences, the algorithm **splits images of the same person into multiple clusters** to better capture subtle variations.\n",
        "\n",
        "\n",
        "\n",
        "###  What does this tell us?\n",
        "\n",
        "- Real-world face images are complex and contain more variation than just the identity of the person.\n",
        "- Clustering is capturing these **intra-person variations** along with inter-person differences.\n",
        "- This highlights the importance of **dimensionality reduction and feature extraction** before clustering or classification tasks.\n",
        "\n",
        "\n",
        "\n",
        "###  Next steps:\n",
        "\n",
        "- You might want to try other clustering algorithms or tweak parameters.\n",
        "- Or move towards supervised learning methods that explicitly learn to recognize identities despite variations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2_cRFjrFrye"
      },
      "source": [
        "### Understanding Inertia in KMeans Clustering\n",
        "\n",
        "**Inertia** is a measure of how well the clustering has grouped the data points.\n",
        "\n",
        "- It is the **sum of squared distances** between each data point and the centroid of the cluster it belongs to.\n",
        "- A **lower inertia value** means that the points are closer to their cluster centers, indicating more compact and well-defined clusters.\n",
        "- However, inertia always decreases as you increase the number of clusters (k), so it should be used together with other metrics like the silhouette score to find a good balance.\n",
        "\n",
        "**What we are trying to do:**  \n",
        "By plotting inertia against different numbers of clusters (`k`), we look for an \"elbow point\" where the inertia stops decreasing significantly. This helps us choose a number of clusters that balances compactness with simplicity, avoiding too many clusters that overfit the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTbkz4ioE6VG"
      },
      "outputs": [],
      "source": [
        "#  Step: Analyze KMeans Clustering Using Inertia (Within-Cluster Sum of Squares)\n",
        "# ------------------------------------------------------------------------------\n",
        "# Inertia measures how tightly grouped the points in each cluster are.\n",
        "# Lower inertia means clusters are more compact.\n",
        "#\n",
        "#  Methods and attributes:\n",
        "#   - model.inertia_: attribute from each fitted KMeans model\n",
        "#   - matplotlib.pyplot for plotting\n",
        "#\n",
        "#  Variables:\n",
        "#   - kmeans_per_k: list of fitted KMeans models for different k values\n",
        "#   - k_range: range/list of k values tested\n",
        "#   - best_index: index of the best model based on silhouette score (from previous step)\n",
        "#\n",
        "#  Your task:\n",
        "# 1. Extract inertia values for each model in kmeans_per_k.\n",
        "# 2. Identify the inertia value of the best model (best_index).\n",
        "# 3. Plot inertia vs k and highlight the best model's inertia.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPPK2lqeGuj2"
      },
      "source": [
        "### Elbow Diagram\n",
        "\n",
        "The **elbow diagram** is a plot of the **inertia** values for different numbers of clusters (`k`) in KMeans clustering.\n",
        "\n",
        "- **Inertia** measures how tightly data points are grouped around their cluster centers (lower is better).\n",
        "- As `k` increases, inertia typically decreases because more clusters mean smaller, tighter groups.\n",
        "- The **elbow point** is where the decrease in inertia slows down significantly.\n",
        "- This \"elbow\" helps us select an optimal number of clusters by balancing compactness and simplicity.\n",
        "\n",
        "If the elbow is not clear, we may need to consider other metrics or domain knowledge to choose `k`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTuxBhgFF6i6"
      },
      "outputs": [],
      "source": [
        "\n",
        "inertias = [model.inertia_ for model in kmeans_per_k]\n",
        "best_inertia = inertias[best_index]\n",
        "\n",
        "plt.figure(figsize=(8, 3.5))\n",
        "plt.plot(k_range, inertias, \"bo-\", label=\"Inertia\")\n",
        "plt.xlabel(\"$k$\", fontsize=14)\n",
        "plt.ylabel(\"Inertia\", fontsize=14)\n",
        "plt.plot(best_k, best_inertia, \"rs\", label=f\"Best k = {best_k}\")\n",
        "plt.legend()\n",
        "plt.title(\"KMeans Inertia for Different Numbers of Clusters\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eD9DBYNGOgY"
      },
      "source": [
        "The optimal number of clusters is not clear on this inertia diagram, as there is no obvious elbow, so let's stick with k=120."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr2PHj4JHVbp"
      },
      "source": [
        "### Visualize the clusters:\n",
        "**do you see similar faces in each cluster?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMDsYbHJIHSQ"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define a function `plot_faces` that:\n",
        "# - Takes the following arguments:\n",
        "#     - `faces`: a NumPy array of flattened face images (shape: [n_samples, 4096])\n",
        "#     - `labels`: corresponding labels or cluster IDs (array-like)\n",
        "#     - `n_cols`: number of columns to display per row (default: 5)\n",
        "# - Inside the function:\n",
        "#     - Use the `reshape` method on `faces` to convert each flattened image to 64x64 pixels\n",
        "#     - Calculate the number of rows needed for the grid layout\n",
        "#     - Use matplotlib's `plt.figure()` to create the figure\n",
        "#     - Loop over the images and labels with `enumerate()` and `zip()`\n",
        "#       - For each face:\n",
        "#         - Use `plt.subplot()` to create a subplot in the grid\n",
        "#         - Display the image with `plt.imshow()`, set `cmap=\"gray\"`\n",
        "#         - Turn off axis ticks with `plt.axis(\"off\")`\n",
        "#         - Set the title with `plt.title()` to show the label\n",
        "#     - Use `plt.show()` to display the full grid of images\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Loop over unique cluster IDs from `best_model.labels_` using `np.unique()`\n",
        "# - For each cluster ID:\n",
        "#     - Create a boolean mask `in_cluster` where `best_model.labels_ == cluster_id`\n",
        "#     - Select corresponding faces from `X_train` using this mask\n",
        "#     - Select corresponding labels from `y_train` using the same mask\n",
        "#     - Call the `plot_faces()` function to visualize the cluster faces with labels\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrMJOUhqGPFp"
      },
      "outputs": [],
      "source": [
        "def plot_faces(faces, labels, n_cols=5):\n",
        "    faces = faces.reshape(-1, 64, 64)\n",
        "    n_rows = (len(faces) - 1) // n_cols + 1\n",
        "    plt.figure(figsize=(n_cols, n_rows * 1.1))\n",
        "    for index, (face, label) in enumerate(zip(faces, labels)):\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(face, cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(label)\n",
        "    plt.show()\n",
        "\n",
        "for cluster_id in np.unique(best_model.labels_):\n",
        "    print(\"Cluster\", cluster_id)\n",
        "    in_cluster = best_model.labels_==cluster_id\n",
        "    faces = X_train[in_cluster]\n",
        "    labels = y_train[in_cluster]\n",
        "    plot_faces(faces, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzCojkWpJC9J"
      },
      "source": [
        "About 2 out of 3 clusters are useful: that is, they contain at least 2 pictures, all of the same person. However, the rest of the clusters have either one or more intruders, or they have just a single picture.\n",
        "\n",
        "Clustering images this way may be too imprecise to be directly useful when training a model (as we will see below), but it can be tremendously useful when labeling images in a new dataset: it will usually make labelling much faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2hejdM_KWBm"
      },
      "source": [
        "## 11. Using Clustering as Preprocessing for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzdJugfiKfAI"
      },
      "source": [
        "Continuing with the Olivetti faces dataset, train a classifier to predict which person is represented in each picture, and evaluate it on the validation set.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l5nRD6pKz5L"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import the RandomForestClassifier from sklearn.ensemble.\n",
        "\n",
        "# Step 2: Create a RandomForestClassifier instance named `clf` with:\n",
        "#         - `n_estimators=150` (number of trees)\n",
        "#         - `random_state=42` for reproducibility.\n",
        "\n",
        "# Step 3: Train the classifier using the `.fit()` method on the PCA-transformed training data:\n",
        "#         - Features: `X_train_pca`\n",
        "#         - Labels: `y_train`\n",
        "\n",
        "# Step 4: Evaluate the trained model using the `.score()` method on the PCA-transformed validation set:\n",
        "#         - Features: `X_valid_pca`\n",
        "#         - Labels: `y_valid`\n",
        "#         - This returns the accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpWfjFYBJDd9"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
        "clf.fit(X_train_pca, y_train)\n",
        "clf.score(X_valid_pca, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPVYzENJK840"
      },
      "source": [
        "Next, use K-Means as a dimensionality reduction tool, and train a classifier on the reduced set.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJt4Jl97LamR"
      },
      "outputs": [],
      "source": [
        "# Step 1: Use the best KMeans model stored in `best_model` to transform your PCA-reduced data.\n",
        "#         This applies clustering-based dimensionality reduction.\n",
        "#         - Transform the training set `X_train_pca` into `X_train_reduced`\n",
        "#         - Transform the validation set `X_valid_pca` into `X_valid_reduced`\n",
        "#         - Transform the test set `X_test_pca` into `X_test_reduced`\n",
        "#         (Hint: Use the `.transform()` method of the KMeans model.)\n",
        "\n",
        "# Step 2: Import `RandomForestClassifier` from `sklearn.ensemble`.\n",
        "\n",
        "# Step 3: Instantiate the classifier `clf` with 150 estimators and a fixed `random_state=42`.\n",
        "\n",
        "# Step 4: Fit the classifier on the reduced training data `X_train_reduced` with labels `y_train` using `.fit()`.\n",
        "\n",
        "# Step 5: Evaluate the model accuracy on the reduced validation data `X_valid_reduced` and labels `y_valid` using `.score()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aUsyKt6K9iZ"
      },
      "outputs": [],
      "source": [
        "X_train_reduced = best_model.transform(X_train_pca)\n",
        "X_valid_reduced = best_model.transform(X_valid_pca)\n",
        "X_test_reduced = best_model.transform(X_test_pca)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
        "clf.fit(X_train_reduced, y_train)\n",
        "\n",
        "clf.score(X_valid_reduced, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_OXZX6wLlBA"
      },
      "source": [
        " That's not better at all! Let's see if tuning the number of clusters helps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak9H_vVvLwKs"
      },
      "source": [
        "We could use a `GridSearchCV` like we did earlier in this notebook, but since we already have a validation set, we don't need K-fold cross-validation, and we're only exploring a single hyperparameter, so it's simpler to just run a loop manually:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnYZeOtoMHPI"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import Pipeline from sklearn.pipeline.\n",
        "\n",
        "# Step 2: For each number of clusters in the list `k_range`, do the following:\n",
        "#   - Create a pipeline named `pipeline` that includes:\n",
        "#       - A KMeans clustering step named \"kmeans\" with `n_clusters` clusters.\n",
        "#       - A RandomForestClassifier step named \"forest_clf\" with 150 trees.\n",
        "#   - Fit the pipeline on the PCA-reduced training data (`X_train_pca`) and training labels (`y_train`) using `.fit()`.\n",
        "#   - Print the current number of clusters (`n_clusters`) and the validation accuracy obtained by scoring on `X_valid_pca` and `y_valid` using `.score()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL4cY2uFLcM1"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "for n_clusters in k_range:\n",
        "    pipeline = Pipeline([\n",
        "        (\"kmeans\", KMeans(n_clusters=n_clusters, random_state=42)),\n",
        "        (\"forest_clf\", RandomForestClassifier(n_estimators=150, random_state=42))\n",
        "    ])\n",
        "    pipeline.fit(X_train_pca, y_train)\n",
        "    print(n_clusters, pipeline.score(X_valid_pca, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfgadD-4Mi46"
      },
      "source": [
        "Oh well, even by tuning the number of clusters, we never get beyond 80% accuracy. Looks like the distances to the cluster centroids are not as informative as the original images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US4h_Z8PN-sI"
      },
      "source": [
        "### Combining PCA Features with Cluster Membership Features\n",
        "\n",
        "In our previous steps, we reduced the dimensionality of the face images using **PCA**, which gives us a compact and important representation of each face. Then, we applied **KMeans clustering** to group similar faces into clusters.\n",
        "\n",
        "But instead of using only the PCA features for classification, we can improve our model by also including information about the cluster each face belongs to. Here’s the idea:\n",
        "\n",
        "1. **PCA features** represent the continuous characteristics of each face image in fewer dimensions.\n",
        "2. **KMeans clustering** groups faces into clusters based on similarity.\n",
        "3. For each face, we find which cluster it belongs to (its cluster label).\n",
        "4. We convert these cluster labels into **one-hot encoded vectors** (e.g., if there are 5 clusters, cluster 2 becomes `[0, 1, 0, 0, 0]`).\n",
        "5. We **append** these one-hot cluster vectors to the PCA features, creating an extended feature set that combines both types of information.\n",
        "6. Finally, we train our classifier using this combined feature set.\n",
        "\n",
        "---\n",
        "\n",
        "### Why does this help?\n",
        "\n",
        "- PCA captures *detailed, continuous* features about each face.\n",
        "- Cluster membership adds *categorical* information about which group of similar faces an image belongs to.\n",
        "- Combining these features provides the classifier with richer information, potentially improving prediction accuracy.\n",
        "\n",
        "Think of it like giving the model both a detailed description and a group label — it can use both to make better decisions!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WE2oi5WMiW9"
      },
      "outputs": [],
      "source": [
        "# Step: Combine the original PCA features with the cluster membership features (one-hot encoded)\n",
        "# Hint: Use numpy's c_ method to concatenate arrays column-wise.\n",
        "# Variables:\n",
        "# - X_train_pca: PCA features for the training set\n",
        "# - X_train_reduced: Cluster membership features (one-hot encoded) for the training set\n",
        "# - Similarly for validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97-wGv2cRpne"
      },
      "outputs": [],
      "source": [
        "X_train_extended = np.c_[X_train_pca, X_train_reduced]\n",
        "X_valid_extended = np.c_[X_valid_pca, X_valid_reduced]\n",
        "X_test_extended = np.c_[X_test_pca, X_test_reduced]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgPwhJlDR7l7"
      },
      "outputs": [],
      "source": [
        "# Step: Train a RandomForestClassifier on the extended feature set\n",
        "# Hint: Use RandomForestClassifier from sklearn.ensemble with 150 trees and random_state=42 for reproducibility\n",
        "# Variables:\n",
        "# - X_train_extended: Training features combining PCA + cluster features\n",
        "# - y_train: Training labels\n",
        "# - X_valid_extended: Validation features combining PCA + cluster features\n",
        "# - y_valid: Validation labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbNEO9i6R1rN"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
        "clf.fit(X_train_extended, y_train)\n",
        "clf.score(X_valid_extended, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh5PXg4gSLNR"
      },
      "source": [
        "That's a bit better, but still worse than without the cluster features. The clusters are not useful to directly train a classifier in this case (but they can still help when labelling new training instances)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6k5OYpwSkUR"
      },
      "source": [
        "## 12. A Gaussian Mixture Model for the Olivetti Faces Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcU0gt8cSsCK"
      },
      "source": [
        "Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the algorithm, you should probably reduce the dataset's dimensionality (e.g., use PCA, preserving 99% of the variance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzICrUnkSL9I"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gm = GaussianMixture(n_components=40, random_state=42)\n",
        "y_pred = gm.fit_predict(X_train_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-6cv5kyS3bc"
      },
      "source": [
        "Use the model to generate some new faces (using the sample() method), and visualize them (if you used PCA, you will need to use its inverse_transform() method)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVqPMBeBSzD2"
      },
      "outputs": [],
      "source": [
        "n_gen_faces = 20\n",
        "gen_faces_reduced, y_gen_faces = gm.sample(n_samples=n_gen_faces)\n",
        "gen_faces = pca.inverse_transform(gen_faces_reduced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuzyHXY6TBm1"
      },
      "outputs": [],
      "source": [
        "plot_faces(gen_faces, y_gen_faces)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjiJ1ALXTIGB"
      },
      "source": [
        "Try to modify some images (e.g., rotate, flip, darken) and see if the model can detect the anomalies (i.e., compare the output of the score_samples() method for normal images and for anomalies)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkWMjJW0TI5N"
      },
      "outputs": [],
      "source": [
        "n_rotated = 4\n",
        "rotated = np.transpose(X_train[:n_rotated].reshape(-1, 64, 64), axes=[0, 2, 1])\n",
        "rotated = rotated.reshape(-1, 64*64)\n",
        "y_rotated = y_train[:n_rotated]\n",
        "\n",
        "n_flipped = 3\n",
        "flipped = X_train[:n_flipped].reshape(-1, 64, 64)[:, ::-1]\n",
        "flipped = flipped.reshape(-1, 64*64)\n",
        "y_flipped = y_train[:n_flipped]\n",
        "\n",
        "n_darkened = 3\n",
        "darkened = X_train[:n_darkened].copy()\n",
        "darkened[:, 1:-1] *= 0.3\n",
        "y_darkened = y_train[:n_darkened]\n",
        "\n",
        "X_bad_faces = np.r_[rotated, flipped, darkened]\n",
        "y_bad = np.concatenate([y_rotated, y_flipped, y_darkened])\n",
        "\n",
        "plot_faces(X_bad_faces, y_bad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGZvJDVNTPgE"
      },
      "outputs": [],
      "source": [
        "X_bad_faces_pca = pca.transform(X_bad_faces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vQ2gTuRTX9M"
      },
      "outputs": [],
      "source": [
        "gm.score_samples(X_bad_faces_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzw3AcZ-Tel_"
      },
      "source": [
        "The bad faces are all considered highly unlikely by the Gaussian Mixture model. Compare this to the scores of some training instances:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_SmzDPDTfZP"
      },
      "outputs": [],
      "source": [
        "gm.score_samples(X_train_pca[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT_WAbSATqri"
      },
      "source": [
        " # Using Dimensionality Reduction Techniques for Anomaly Detection\n",
        "\n",
        "\n",
        " In this section, we use Principal Component Analysis (PCA) to reconstruct images and measure how well the model can represent each face. By comparing the original images with their PCA reconstructions, we can identify anomalies or unusual images — those that PCA struggles to represent accurately.\n",
        "\n",
        "The reconstruction error (mean squared error between the original and reconstructed image) is a key metric:\n",
        "\n",
        "Low error indicates that the image fits well within the learned patterns (normal faces).\n",
        "\n",
        "High error suggests the image may be noisy, corrupted, or an outlier (anomalous face).\n",
        "\n",
        "This technique is useful in real-world applications such as:\n",
        "\n",
        "Detecting corrupted or poor-quality images in biometric security systems.\n",
        "\n",
        "Filtering out invalid or unusual data before training face recognition models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slHR6CEcT4fD"
      },
      "source": [
        "Some dimensionality reduction techniques can also be used for anomaly detection. For example, take the Olivetti faces dataset and reduce it with PCA, preserving 99% of the variance. Then compute the reconstruction error for each image. Next, take some of the modified images you built in the previous exercise, and look at their reconstruction error: notice how much larger the reconstruction error is. If you plot a reconstructed image, you will see why: it tries to reconstruct a normal face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCjAGwIxT-Q3"
      },
      "source": [
        "We already reduced the dataset using PCA earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U6ZP2JcTsuV"
      },
      "outputs": [],
      "source": [
        "X_train_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NDgIDNjUTGH"
      },
      "outputs": [],
      "source": [
        "# Step: Define a function to compute reconstruction errors using PCA\n",
        "# Hint:\n",
        "# - Use pca.transform() to reduce dimensionality of input data X\n",
        "# - Use pca.inverse_transform() to reconstruct the original data from reduced data\n",
        "# - Compute mean squared error (MSE) between reconstructed and original data for each sample\n",
        "# Variables:\n",
        "# - pca: fitted PCA object\n",
        "# - X: input data to compute reconstruction error on\n",
        "# Returns:\n",
        "# - mse: array of reconstruction errors for each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BK4iOmFUUDN"
      },
      "outputs": [],
      "source": [
        "def reconstruction_errors(pca, X):\n",
        "    X_pca = pca.transform(X)\n",
        "    X_reconstructed = pca.inverse_transform(X_pca)\n",
        "    mse = np.square(X_reconstructed - X).mean(axis=-1)\n",
        "    return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G3bV5WxUoxb"
      },
      "outputs": [],
      "source": [
        "# Step: Calculate the average reconstruction error on the training set\n",
        "# Hint:\n",
        "# - Use the reconstruction_errors() function you defined earlier\n",
        "# - Pass the fitted PCA object 'pca' and training data 'X_train' as arguments\n",
        "# - Use .mean() to get the average reconstruction error across all training samples\n",
        "# Variables:\n",
        "# - pca: the fitted PCA object\n",
        "# - X_train: training dataset (original features)\n",
        "# - Output: average reconstruction error (a single float number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpGZglMhUev-"
      },
      "outputs": [],
      "source": [
        "reconstruction_errors(pca, X_train).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4ziyCboUyS7"
      },
      "outputs": [],
      "source": [
        "# Step: Calculate the average reconstruction error for a set of \"bad\" or noisy face images\n",
        "# Hint:\n",
        "# - Use the previously defined function reconstruction_errors()\n",
        "# - Pass the fitted PCA object 'pca' and the dataset 'X_bad_faces' as arguments\n",
        "# - Calculate the mean error to summarize the overall reconstruction quality\n",
        "# Variables:\n",
        "# - pca: the fitted PCA object\n",
        "# - X_bad_faces: dataset containing distorted or noisy faces to test the PCA reconstruction\n",
        "# - Output: average reconstruction error for these \"bad\" faces (a single float number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxT_k7fEU3Iu"
      },
      "outputs": [],
      "source": [
        "reconstruction_errors(pca, X_bad_faces).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOdA6gUXVDZh"
      },
      "outputs": [],
      "source": [
        "# Step: Visualize the \"bad\" or noisy face images using the provided plot_faces() function\n",
        "# Hint:\n",
        "# - Use the function plot_faces()\n",
        "# - Pass the dataset 'X_bad_faces' and the corresponding labels 'y_bad'\n",
        "# - This will help you visually inspect how the noisy faces look compared to normal ones\n",
        "# Variables:\n",
        "# - X_bad_faces: the set of noisy or distorted face images\n",
        "# - y_bad: the corresponding labels or IDs for these images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_amAxowFU81f"
      },
      "outputs": [],
      "source": [
        "plot_faces(X_bad_faces, y_bad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHecEx6sVOgh"
      },
      "outputs": [],
      "source": [
        "# Step: Reconstruct the \"bad\" faces from their PCA representation\n",
        "# Hint:\n",
        "# - Use the PCA method `inverse_transform()` to convert reduced data back to original space\n",
        "# - Store the reconstructed faces in the variable `X_bad_faces_reconstructed`\n",
        "# Variables:\n",
        "# - X_bad_faces_pca: PCA-transformed representation of the noisy faces\n",
        "# - X_bad_faces_reconstructed: reconstructed faces after inverse PCA transform\n",
        "\n",
        "\n",
        "# Step: Visualize the reconstructed \"bad\" faces to compare with original noisy images\n",
        "# Hint:\n",
        "# - Use the `plot_faces()` function again\n",
        "# - Pass `X_bad_faces_reconstructed` and `y_bad` as inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvhTxhQXWrim"
      },
      "source": [
        "PCA Reconstruction of Faces\n",
        "In this step, we take the PCA-reduced representation of the “bad” or noisy face images (X_bad_faces_pca) and reconstruct them back into the original high-dimensional space using the PCA method inverse_transform(). This process attempts to recover the original images from their compressed form.\n",
        "\n",
        "Why do we do this?\n",
        "PCA reduces the dimensionality of the data by keeping only the most important features. When we reconstruct the images, we can see how much information was retained or lost after this compression.\n",
        "\n",
        "What does the visualization show?\n",
        "By plotting the reconstructed images (X_bad_faces_reconstructed), we can visually compare them with the original noisy images (X_bad_faces). This helps us understand the quality of the dimensionality reduction and how well PCA captures the essential features of the faces despite noise or distortions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlutcHNeWskq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
