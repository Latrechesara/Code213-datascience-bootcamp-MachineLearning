{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align:left;\">\n",
        "  <a href=\"https://code213.tech/\" target=\"_blank\">\n",
        "    <img src=\"../images/code213.PNG\" alt=\"QWorld\">\n",
        "  </a>\n",
        "  <p><em>prepared by Latreche Sara</em></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhrWR3V9Kh4p"
      },
      "source": [
        "# Setting up the environement\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDKQcUm3Jq1f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "\n",
        "font = {'size'   : 16}\n",
        "matplotlib.rc('font', **font)\n",
        "matplotlib.rc('xtick', labelsize=14)\n",
        "matplotlib.rc('ytick', labelsize=14)\n",
        "#matplotlib.rcParams.update({'figure.autolayout': True})\n",
        "matplotlib.rcParams['figure.dpi'] = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru4cipEHKsPu"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPbAMXs8Kr9p"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfwk_EEmKxIw"
      },
      "outputs": [],
      "source": [
        "sel_features = pd.read_csv('sel_features.csv', sep = '\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA7U7_FeK3QV"
      },
      "outputs": [],
      "source": [
        "sel_target = pd.read_csv('sel_target.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldB_1ncoK6KG"
      },
      "outputs": [],
      "source": [
        "sel_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C5TfG9gK9L5"
      },
      "outputs": [],
      "source": [
        "sel_target.values.ravel() #changes shape to 1d row-like array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVL6qhDKLEmn"
      },
      "source": [
        "### Let's start with Random Forests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6pn0M_WK_1-"
      },
      "outputs": [],
      "source": [
        "# Initialize a Random Forest model with 200 trees and a maximum of 4 features per split\n",
        "# Set the random_state for reproducible results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1H3t_9sL26z"
      },
      "source": [
        "**After** the model has been fit, it will have the attribute \"feature\\_importances\\_\". We can look at the feature importance using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YW3He-mLxMp"
      },
      "outputs": [],
      "source": [
        "# Train the model on the selected features and flattened target values(sel_target.values.ravel())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7fys5-oMIZ7"
      },
      "outputs": [],
      "source": [
        "# Get the importance of each feature used by the trained Random Forest model\n",
        "\n",
        "# Preview the importances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVCadpCKMibF"
      },
      "source": [
        "The code below plots the feature importances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z89G9eapMiIs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Step 2: Get the feature importances from the trained model\n",
        "# This gives a score for each feature, the higher the score the more important the feature\n",
        "\n",
        "# Step 3: Get the indices of features sorted by importance (in descending order)\n",
        "\n",
        "# Step 4: Print the ranking of the features\n",
        "\n",
        "\n",
        "# Step 5: Visualize the feature importances using a bar chart\n",
        "\n",
        "\n",
        "# Plot bar chart with the features sorted by importance\n",
        "\n",
        "\n",
        "# Add the feature names on the x-axis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsUvg0E3OCdZ"
      },
      "source": [
        "### In this problem, all the features are quite important, which is somewhat expected because there are only 6 of them. In many other cases, when we have more features with a high degree of redundancy, we'll find that many features have negligible importance, and this can be useful as a feature selection tool.\n",
        "\n",
        "### Something we can do is to compare with the results of other algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U64v9U97N1Oc"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Initialize the plotting environment\n",
        "\n",
        "\n",
        "# Step 2: Define the models to compare\n",
        "# Each model is initialized with specific hyperparameters\n",
        "\n",
        "\n",
        "# Step 3: Define model names for labeling\n",
        "\n",
        "# Step 4: Loop through each model to compute and plot feature importances\n",
        "\n",
        "    # Fit model to the selected features and target\n",
        "\n",
        "    # Extract feature importances from the trained model\n",
        "\n",
        "    # Sort the features by importance (descending)\n",
        "\n",
        "    # Plot the feature importances using slightly shifted bars for each model\n",
        "\n",
        "\n",
        "# Step 5: Final plot customization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiiTEFSXPH4T"
      },
      "source": [
        "###  Reminder: Interpreting Feature Importance\n",
        "\n",
        "This is a reminder that **feature importance is only an indication** and is often **algorithm-dependent**.\n",
        "\n",
        "**Take-home message**:  \n",
        "These methods are usually quite reliable if you want to **select a set of features that help reach a certain performance**,  \n",
        "but the **ranking of individual features is more uncertain**.\n",
        "\n",
        " **Important Note on Data Leakage**:  \n",
        "If you perform feature selection or ranking using the **entire dataset** (before splitting into train/test sets),  \n",
        "you introduce **data leakage** â€” the model effectively \"sees\" the test set and selects features that work best on it.  \n",
        "To avoid this, a **better approach** is to:\n",
        "- Perform the ranking multiple times on different **train/test splits** (e.g., using cross-validation),\n",
        "- Then select the features that are **consistently ranked as important**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yGET58tOmLH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDtgssPZMbl7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
