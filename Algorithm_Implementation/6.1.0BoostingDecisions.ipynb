{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align:left;\">\n",
        "  <a href=\"https://code213.tech/\" target=\"_blank\">\n",
        "    <img src=\"../images/code213.PNG\" alt=\"QWorld\">\n",
        "  </a>\n",
        "  <p><em>prepared by Latreche Sara</em></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbvghKWpIKbR"
      },
      "source": [
        "Step1: setting up the envirement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIANM0JPIJOC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "font = {'size'   : 16}\n",
        "matplotlib.rc('font', **font)\n",
        "matplotlib.rc('xtick', labelsize=14)\n",
        "matplotlib.rc('ytick', labelsize=14)\n",
        "#matplotlib.rcParams.update({'figure.autolayout': True})\n",
        "matplotlib.rcParams['figure.dpi'] = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A2CZHIkIQ1y"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_validate, KFold, cross_val_predict, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZen4mfRIU-B"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import the pandas library to handle tabular data.\n",
        "# Use: import pandas as pd\n",
        "\n",
        "# Step 2: Read the input feature file.\n",
        "# Use read_csv method with tab separator ('\\t') since the columns are separated by tabs.\n",
        "# Save the data into a variable named sel_features\n",
        "# File name: 'sel_features.csv'\n",
        "sel_features= pd.read_csv(\"sel_features\",sep=\"'\\'\")\n",
        "# Step 3: Read the target redshift values.\n",
        "# Use read_csv with default settings (it's comma-separated by default).\n",
        "# Save the data into a variable named sel_target\n",
        "# File name: 'sel_target.csv'\n",
        "\n",
        "# Step 4: Check the shape (number of rows and columns) of the sel_features data\n",
        "# This helps verify how many galaxy observations and feature columns we have.\n",
        "# Use the .shape attribute on sel_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olvc-gGCPXL4"
      },
      "outputs": [],
      "source": [
        "# Convert the target DataFrame to a 1D NumPy array using .values.ravel()\n",
        "# This flattens the redshift column into a row-like array, which is the expected format for training a model\n",
        "# Use this for your target variable y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaTSCeuua24v"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import AdaBoostRegressor from sklearn.ensemble\n",
        "# Step 2: Create an instance of AdaBoostRegressor using default parameters\n",
        "\n",
        "# Step 3: Import cross_val_predict from sklearn.model_selection\n",
        "# Step 4: Use cross_val_predict to perform 5-fold cross-validation\n",
        "#         - Pass in your model, the feature data (sel_features), and the target data\n",
        "#         - Make sure to flatten the target array using .values.ravel()\n",
        "#         - Use KFold with 5 splits, shuffle=True, and a fixed random_state for reproducibility\n",
        "#         - Store the cross-validated predictions in a variable (e.g., ypred)\n",
        "\n",
        "# Step 5: Import matplotlib.pyplot as plt\n",
        "# Step 6: Create a scatter plot of true redshift values vs predicted values\n",
        "#         - Use sel_target for the x-axis and ypred for the y-axis\n",
        "#         - Use a small point size (s=10) for clarity\n",
        "\n",
        "# Step 7: Set the plot size to 7x7 inches\n",
        "# Step 8: Set the x-axis and y-axis limits from 0 to 3 to focus on the most relevant redshift range\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzXPF6hPbrUN"
      },
      "source": [
        "What it visually tells you:\n",
        "It helps you quickly evaluate how well your model is performing. If the points are close to the diagonal, it means your model is accurately predicting redshifts. If they are spread out, especially far from that line, there's room for improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJkZpwSfb0_x"
      },
      "source": [
        "### This is where we started wondering whether the boosting process was working\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMu7UuGXbz2H"
      },
      "outputs": [],
      "source": [
        "# Use the method get_params() on the AdaBoostRegressor model to retrieve its current parameters.\n",
        "# This shows the default or modified settings such as number of estimators and learning rate.\n",
        "# It helps you understand or tweak how the model behaves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjyMLo6vdkPu"
      },
      "source": [
        "ere we use AdaBoostRegressor which by default uses shallow decision trees (max_depth=3) as weak learners.\n",
        "AdaBoost builds an ensemble by sequentially training these weak learners and adaptively reweighting training samples,\n",
        "focusing more on those samples that previous learners mispredicted.\n",
        "This is different from naive stacking; the adaptive boosting process is key to its performance.\n",
        "This approach is inspired by the scikit-learn AdaBoost example:\n",
        "https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0t3KXiod9gW"
      },
      "source": [
        "#### This is what happens if max_depth = 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PArk0nXBbSsp"
      },
      "outputs": [],
      "source": [
        "# Create the dataset\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "rng = np.random.RandomState(1)\n",
        "X = np.linspace(0, 4, 100)[:, np.newaxis]\n",
        "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
        "\n",
        "weakl = DecisionTreeRegressor(max_depth=3)\n",
        "\n",
        "# Fit regression model, saving each \"stage\"\n",
        "\n",
        "regr_1 = weakl\n",
        "\"\"\n",
        "regr_2 = AdaBoostRegressor(weakl,\n",
        "                          n_estimators=2, random_state=rng)\n",
        "\n",
        "regr_3 = AdaBoostRegressor(weakl,\n",
        "                          n_estimators=3, random_state=rng)\n",
        "\n",
        "regr_4 = AdaBoostRegressor(weakl,\n",
        "                          n_estimators=4, random_state=rng)\n",
        "\n",
        "regr_10 = AdaBoostRegressor(weakl,\n",
        "                          n_estimators=10, random_state=rng)\n",
        "\n",
        "regr_100 = AdaBoostRegressor(weakl,\n",
        "                          n_estimators=100, random_state=rng)\n",
        "\n",
        "\n",
        "regr_1.fit(X, y)\n",
        "regr_2.fit(X, y)\n",
        "regr_3.fit(X, y)\n",
        "regr_4.fit(X, y)\n",
        "regr_10.fit(X, y)\n",
        "regr_100.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "y_1 = regr_1.predict(X)\n",
        "y_2 = regr_2.predict(X)\n",
        "y_3 = regr_3.predict(X)\n",
        "y_4 = regr_4.predict(X)\n",
        "y_10 = regr_10.predict(X)\n",
        "\n",
        "for yp in [y_1,y_2,y_3,y_4,y_10]:\n",
        "    print('r2 score: ', np.round(metrics.r2_score(yp,y),3))\n",
        "\n",
        "# Plot the results\n",
        "\n",
        "plt.scatter(X, y, c=\"k\", s=10,label=\"training samples\")\n",
        "plt.plot(X, y_1, \"-g\", label=\"n_estimators=1\", linewidth=1)\n",
        "plt.plot(X, y_2, \"--r\", label=\"n_estimators=2\", linewidth=1)\n",
        "plt.plot(X, y_3, \"-.b\", label=\"n_estimators=3\", linewidth=1)\n",
        "plt.xlabel(\"data\")\n",
        "plt.ylabel(\"target\")\n",
        "plt.title(\"AdaBoost Regression, max depth = 3\", fontsize = 14)\n",
        "plt.legend(fontsize=10);\n",
        "#plt.tight_layout()\n",
        "#plt.savefig(\"AdaBoost_3.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oTcWL-jfYbU"
      },
      "source": [
        "#### This is what happens if max_depth = 6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJVrKh3FcI74"
      },
      "outputs": [],
      "source": [
        "# Create the dataset\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "rng = np.random.RandomState(1)\n",
        "X = np.linspace(0, 4, 100)[:, np.newaxis]\n",
        "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
        "\n",
        "weakl = DecisionTreeRegressor(max_depth=6)\n",
        "\n",
        "# Fit regression model, saving each \"stage\"\n",
        "regr_1 = weakl\n",
        "\"\"\n",
        "regr_2 = AdaBoostRegressor(weakl,\n",
        "                          n_estimators=2, random_state=rng)\n",
        "\n",
        "regr_3 = AdaBoostRegressor(weakl,\n",
        "                          n_estimators=3, random_state=rng)\n",
        "\n",
        "regr_4 = AdaBoostRegressor(weakl,\n",
        "                          n_estimators=4, random_state=rng)\n",
        "\n",
        "regr_10 = AdaBoostRegressor(weakl,\n",
        "                          n_estimators=10, random_state=rng)\n",
        "\n",
        "regr_100 = AdaBoostRegressor(weakl,\n",
        "                          n_estimators=100, random_state=rng)\n",
        "\n",
        "\n",
        "regr_1.fit(X, y)\n",
        "regr_2.fit(X, y)\n",
        "regr_3.fit(X, y)\n",
        "regr_4.fit(X, y)\n",
        "regr_10.fit(X, y)\n",
        "regr_100.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "y_1 = regr_1.predict(X)\n",
        "y_2 = regr_2.predict(X)\n",
        "y_3 = regr_3.predict(X)\n",
        "y_4 = regr_4.predict(X)\n",
        "y_10 = regr_10.predict(X)\n",
        "\n",
        "for yp in [y_1,y_2,y_3,y_4,y_10]:\n",
        "    print(metrics.r2_score(yp,y))\n",
        "\n",
        "# Plot the results\n",
        "\n",
        "plt.scatter(X, y, c=\"k\", s=10,label=\"training samples\")\n",
        "plt.plot(X, y_1, \"-g\", label=\"n_estimators=1\", linewidth=1)\n",
        "plt.plot(X, y_2, \"--r\", label=\"n_estimators=2\", linewidth=1)\n",
        "plt.plot(X, y_3, \"-.b\", label=\"n_estimators=3\", linewidth=1)\n",
        "plt.xlabel(\"data\")\n",
        "plt.ylabel(\"target\")\n",
        "plt.title(\"AdaBoost Regression, max depth = 6\", fontsize = 14)\n",
        "plt.legend(fontsize=10);\n",
        "#plt.tight_layout()\n",
        "#plt.savefig(\"AdaBoost_6.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjLnrttVf0hj"
      },
      "source": [
        " We can now go back to the photo-z determination problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L7fuJzpgGoM"
      },
      "source": [
        "We create a train/test split because we need to use the \".fit\" method in order to access the \"staged_predict\" property to examine how the prediction changes at each stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suIbAJ-qf1BA"
      },
      "outputs": [],
      "source": [
        "# Step 1: Split the dataset into training and testing sets\n",
        "# - Use the train_test_split method to divide sel_features and sel_target.values.ravel() training and test parts\n",
        "# - Set test_size=0.3 to allocate 30% of data for testing and 70% for training\n",
        "# - Use random_state=42 to ensure reproducibility of the split\n",
        "# We do this so we can train the model on training data and later evaluate on unseen test data\n",
        "# This also allows us to use the \".fit\" method on training data, which is necessary to access the \"staged_predict\" property\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTe_UjqygfS_"
      },
      "source": [
        "We begin with a very weak learner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G1cGHFSgZ04"
      },
      "outputs": [],
      "source": [
        "# Step 2: Create an AdaBoost regressor model\n",
        "# - Use AdaBoostRegressor from sklearn.ensemble\n",
        "# - Set the base estimator to be a DecisionTreeRegressor with max_depth=3 (a weak learner)\n",
        "# - Set n_estimators=30 to use 30 boosting rounds (number of weak learners combined)\n",
        "# This model will sequentially train 30 decision trees, each trying to correct errors of the previous ones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72jFzykkgqQh"
      },
      "outputs": [],
      "source": [
        "# Step 3: Train (fit) the AdaBoost model on the training data\n",
        "# - Use the .fit() method on the model object\n",
        "# - Pass the training features (X_train) and training targets (y_train)\n",
        "# This trains the ensemble of weak learners on the training set to learn the relationship between features and target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlOM3xKuhVWY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plp4-exDhWyl"
      },
      "source": [
        "We can plot the R2 score and the Spearman correlation coefficient between true and predicted values as a function of the number of stages/iterations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3tx7kJrkXWh"
      },
      "source": [
        "## Definitions\n",
        "\n",
        "### R² Score (Coefficient of Determination)\n",
        "- **What is it?**  \n",
        "  Measures how well the predicted values explain the variability of the actual data.\n",
        "\n",
        "- **Formula:**  \n",
        "  $$\n",
        "  R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
        "  $$\n",
        "  where:  \n",
        "  $y_i$ = true value,  \n",
        "  $\\hat{y}_i$ = predicted value,  \n",
        "  $\\bar{y}$ = mean of true values.\n",
        "\n",
        "- **Interpretation:**  \n",
        "  - 1 = perfect prediction  \n",
        "  - 0 = model predicts as well as mean  \n",
        "  - Negative = worse than mean prediction\n",
        "\n",
        "---\n",
        "\n",
        "### Spearman Correlation Coefficient ($\\rho$)\n",
        "- **What is it?**  \n",
        "  Measures the strength and direction of the monotonic relationship between two ranked variables.\n",
        "\n",
        "- **Formula:**  \n",
        "  $$\n",
        "  \\rho = 1 - \\frac{6 \\sum d_i^2}{n (n^2 - 1)}\n",
        "  $$\n",
        "  where:  \n",
        "  $d_i = R(x_i) - S(y_i)$ is the difference between the ranks of corresponding variables,  \n",
        "  $n$ = number of data points.\n",
        "\n",
        "- **Interpretation:**  \n",
        "  - 1 = perfect positive correlation of ranks  \n",
        "  - 0 = no correlation  \n",
        "  - -1 = perfect negative correlation of ranks\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3labl4rhPCu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL-iEVOmhRzw"
      },
      "outputs": [],
      "source": [
        "n_estimators = 30\n",
        "\n",
        "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2 score')\n",
        "\n",
        "plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r')\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "\n",
        "plt.ylim(0,1.0)\n",
        "\n",
        "plt.title('Max depth = 3')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5gqZVkviB8D"
      },
      "source": [
        "### The scores don't seem to improve as we stack more estimators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8KjYouAiI3E"
      },
      "source": [
        "### We can now try again with a stronger base learner (max_depth = 6).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXtd_6MbiCcg"
      },
      "outputs": [],
      "source": [
        "# Number of boosting stages (iterations)\n",
        "n_estimators = 30\n",
        "\n",
        "# Define the AdaBoost regressor model\n",
        "# Using a DecisionTreeRegressor with max_depth=6 as the base learner (stronger tree)\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets (70% train, 30% test)\n",
        "# Use random_state=42 for reproducibility\n",
        "\n",
        "\n",
        "# Fit the AdaBoost model on the training data\n",
        "\n",
        "# For each boosting iteration (stage), calculate the R2 score on the test set predictions\n",
        "\n",
        "# For each boosting iteration, calculate the Spearman correlation between true and predicted test values\n",
        "\n",
        "# Plot the R2 score and Spearman correlation as functions of the boosting iteration number\n",
        "plt.plot(range(n_estimators), r2_scores, label='R2 Score')\n",
        "plt.plot(range(n_estimators), spearman_scores, label='Spearman Correlation')\n",
        "\n",
        "# Add labels and title to the plot\n",
        "plt.xlabel('Iteration')\n",
        "plt.title('AdaBoost Regression Performance\\nBase Estimator: Decision Tree (max_depth=6)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX2k81Vyi8OM"
      },
      "source": [
        "### And do the same with an even stronger base learner (max_depth = 10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hprFgflRiyVR"
      },
      "outputs": [],
      "source": [
        "n_estimators = 30\n",
        "\n",
        "model= AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),\n",
        "                  n_estimators=30)\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2')\n",
        "\n",
        "plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r')\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "\n",
        "plt.title('Base estimator, max depth = 10')\n",
        "\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKH1zrx5jUs-"
      },
      "source": [
        "1. R2 Score Plot\n",
        "What it shows:\n",
        "The R2 score measures how well your model's predictions match the actual data in terms of explained variance.\n",
        "\n",
        "R2 = 1 means perfect prediction.\n",
        "\n",
        "R2 = 0 means the model predicts no better than simply predicting the mean.\n",
        "\n",
        "Negative R2 means the model performs worse than predicting the mean.\n",
        "\n",
        "How to interpret:\n",
        "\n",
        "If the R2 score increases as the number of iterations grows, it means the model is learning and improving its predictions.\n",
        "\n",
        "If the R2 plateaus or decreases, adding more trees doesn’t improve or may even harm the model (overfitting or noise).\n",
        "\n",
        "A smooth increase followed by flattening is typical — the model improves initially but eventually gains less from more estimators.\n",
        "\n",
        "2. Spearman Correlation Plot\n",
        "What it shows:\n",
        "The Spearman correlation measures how well the predicted values preserve the order or ranking of the actual values (regardless of exact numeric values).\n",
        "\n",
        "Spearman = 1 means perfect rank agreement.\n",
        "\n",
        "Spearman = 0 means no correlation in ranks.\n",
        "\n",
        "How to interpret:\n",
        "\n",
        "An increasing Spearman correlation means your model is getting better at correctly ranking the outputs — important if the order matters more than exact values (e.g., prioritizing cases).\n",
        "\n",
        "Like R2, a plateau or decrease suggests limited or negative returns from more estimators.\n",
        "\n",
        "Putting It Together\n",
        "If both R2 and Spearman increase over iterations, the model is improving both in prediction accuracy and ranking quality.\n",
        "\n",
        "If R2 improves but Spearman doesn’t (or vice versa), the model might be improving numeric accuracy but not ranking, or vice versa.\n",
        "\n",
        "If both flatten or decline, adding more trees isn’t helping and might lead to overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfDKNOVCjbLS"
      },
      "source": [
        "### Let's combine all of them in one figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlEqQdK9jBaF"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "n_estimators = 30\n",
        "\n",
        "for i, md in enumerate([3,6,10]):\n",
        "\n",
        "    model = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=md),\n",
        "                  n_estimators=n_estimators)\n",
        "\n",
        "    model.fit(X_train,y_train)\n",
        "\n",
        "    plt.subplot(1,3,i+1)\n",
        "\n",
        "    plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2 score', c = 'steelblue')\n",
        "\n",
        "    plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r', c = 'fuchsia')\n",
        "\n",
        "    plt.xlabel('Iteration')\n",
        "\n",
        "    plt.ylim(0,1.0)\n",
        "\n",
        "    plt.title('Max depth = '+str(md)+', AdaBoost')\n",
        "\n",
        "    if i == 2:\n",
        "        plt.legend();\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "#plt.savefig('AdaB_performance.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unMgourJjsbb"
      },
      "source": [
        "### We sort-of have an answer from the third panel of the figure above, but we could also ask whether we should keep boosting (i.e. if adding more stages is beneficial.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmrxybagjjVs"
      },
      "outputs": [],
      "source": [
        "#Shall we keep boosting? (max_depth = 10)\n",
        "\n",
        "n_estimators = 60\n",
        "\n",
        "model= AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),\n",
        "                  n_estimators=n_estimators)\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2')\n",
        "\n",
        "plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r')\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "\n",
        "plt.title('Base estimator, max depth = 10')\n",
        "\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "024jisi9ki2o"
      },
      "source": [
        "### Conclusions:\n",
        "\n",
        "1.   Stacking learners that are too weak doesn't help;\n",
        "2.   There is a plateau in the boosting stages so that adding more estimators is not beneficial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pd2yv9ikokK"
      },
      "source": [
        "### Would this be true also for Gradient Boosted Trees algorithms?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9zv9dV7jxwQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CUWRn5Jkzs1"
      },
      "source": [
        ":The parameters depend on the particular implementation.\n",
        "\n",
        "In the sklearn formulation, the parameters of each tree are essentially the same we have for Random Forests; additionally we have the \"learning_rate\" parameter, which dictates how much each tree contribute to the final estimator, and the \"subsample\" parameters, which allows one to use a < 1.0 fraction of samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l-QlQ5Bk3ws"
      },
      "source": [
        "We can check how this works with a weak learner on our toy data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1X_1vdfksDR"
      },
      "outputs": [],
      "source": [
        "# Create the dataset\n",
        "\n",
        "plt.figure(figsize=(15,12))\n",
        "\n",
        "rng = np.random.RandomState(1)\n",
        "X = np.linspace(0, 4, 100)[:, np.newaxis]\n",
        "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
        "\n",
        "weakl = DecisionTreeRegressor(max_depth=3)\n",
        "\n",
        "# Fit regression model\n",
        "regr_1 = weakl\n",
        "\"\"\n",
        "regr_2 = GradientBoostingRegressor(max_depth=3,\n",
        "                          n_estimators=2, random_state=rng)\n",
        "\n",
        "regr_3 = GradientBoostingRegressor(max_depth=3,\n",
        "                          n_estimators=3, random_state=rng)\n",
        "\n",
        "regr_4 = GradientBoostingRegressor(max_depth=3,\n",
        "                          n_estimators=4, random_state=rng)\n",
        "regr_10 = GradientBoostingRegressor(max_depth=3,\n",
        "                          n_estimators=10, random_state=rng)\n",
        "\n",
        "regr_100 = GradientBoostingRegressor(max_depth=3,\n",
        "                          n_estimators=100, random_state=rng)\n",
        "\n",
        "\n",
        "regr_1.fit(X, y)\n",
        "regr_2.fit(X, y)\n",
        "regr_3.fit(X, y)\n",
        "regr_4.fit(X, y)\n",
        "regr_10.fit(X, y)\n",
        "regr_100.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "y_1 = regr_1.predict(X)\n",
        "y_2 = regr_2.predict(X)\n",
        "y_3 = regr_3.predict(X)\n",
        "y_4 = regr_4.predict(X)\n",
        "y_10 = regr_10.predict(X)\n",
        "y_100 = regr_100.predict(X)\n",
        "\n",
        "for yp in [y_1,y_2,y_3,y_4,y_10, y_100]:\n",
        "    print('R2 score: ', np.round(metrics.r2_score(yp,y),3))\n",
        "\n",
        "# Plot the results\n",
        "\n",
        "plt.scatter(X, y, c=\"k\", s=10,label=\"training samples\")\n",
        "plt.plot(X, y_1, \"-g\", label=\"n_estimators=1\", linewidth=1)\n",
        "plt.plot(X, y_3, \"-.b\", label=\"n_estimators=3\", linewidth=1)\n",
        "plt.plot(X, y_10, \"-k\", label=\"n_estimators=10\", linewidth=1)\n",
        "plt.plot(X, y_100, \"-c\", label=\"n_estimators=100\", linewidth=1)\n",
        "plt.xlabel(\"data\")\n",
        "plt.ylabel(\"target\")\n",
        "plt.ylim(-2.5,2.5)\n",
        "plt.title(\"Gradient Boosting Regression, max depth = 3\", fontsize = 14)\n",
        "plt.legend(fontsize=14, loc = 'upper right');\n",
        "#plt.tight_layout()\n",
        "#plt.savefig(\"GradBoost_3.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKoo4sHilYdV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "n_estimators = 30\n",
        "\n",
        "# Loop over different max_depth values to compare their effect on model performance\n",
        "\n",
        "    # Create a Gradient Boosting Regressor with current max_depth and fixed n_estimators\n",
        "\n",
        "    # Fit the model on the training data\n",
        "\n",
        "    # Create subplot for each max_depth setting\n",
        "    plt.subplot(1,3,i+1)\n",
        "\n",
        "    # Plot R² score vs number of iterations\n",
        "\n",
        "\n",
        "    # Plot Spearman correlation coefficient vs number of iterations\n",
        "\n",
        "\n",
        "    # Only add legend to the last subplot\n",
        "\n",
        "\n",
        "# Uncomment to save figure to a PDF file\n",
        "# plt.savefig('GBR_performance.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aZKxgu4lBQh"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "n_estimators = 30\n",
        "\n",
        "for i, md in enumerate([3,6,10]):\n",
        "\n",
        "    model = GradientBoostingRegressor(max_depth=md,\n",
        "                  n_estimators=n_estimators)\n",
        "\n",
        "    model.fit(X_train,y_train)\n",
        "\n",
        "    plt.subplot(1,3,i+1)\n",
        "\n",
        "    plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2 score', c = 'steelblue')\n",
        "\n",
        "    plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r', c = 'fuchsia')\n",
        "\n",
        "    plt.xlabel('Iteration')\n",
        "\n",
        "    plt.ylim(0,1.0)\n",
        "\n",
        "    plt.title('Max depth = '+str(md)+', GBR')\n",
        "\n",
        "    if i == 2:\n",
        "        plt.legend();\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "#plt.savefig('GBR_performance.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02nYgr_MlrND"
      },
      "source": [
        "### Because of the different boosting process, GBT models tend to work well even with weak base learners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q8zCRNNlvUo"
      },
      "source": [
        "We compare the performance of AdaBoost and various GBT models on the photometric redshifts problem in the next notebook (FlavorsOfBoosting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfKVX3DclImK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
