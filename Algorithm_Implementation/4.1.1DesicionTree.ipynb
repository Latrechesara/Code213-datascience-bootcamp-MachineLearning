{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ba7957",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left;\">\n",
    "  <a href=\"https://code213.tech/\" target=\"_blank\">\n",
    "    <img src=\"../images/code213.PNG\" alt=\"QWorld\">\n",
    "  </a>\n",
    "  <p><em>prepared by Latreche Sara</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc4223-0c12-4898-945e-7408a3e606dd",
   "metadata": {},
   "source": [
    "# Decision Tree from scratch - A Beginner-Friendly guide\n",
    "## ðŸŒ³ Learning goals\n",
    "\n",
    "\n",
    "In this notebook, we'll:\n",
    "- Learn how to calculate entropy and information gain\n",
    "- Build a simple decision tree from scratch (binary classification)\n",
    "- Compare with scikit-learn's DecisionTreeClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa91ae8-48db-4503-8917-d22c9c51602c",
   "metadata": {},
   "source": [
    "## Decision Tree: Theory \n",
    "\n",
    "## 1. Introduction\n",
    "A **decision tree** is a supervised learning algorithm used for both classification and regression. It splits data based on feature thresholds to maximize **information gain**, eventually forming a tree where each internal node represents a decision, and each leaf node represents a class label or prediction.\n",
    "\n",
    "##  2. Key Mathematical Concepts\n",
    "\n",
    "###  Entropy\n",
    "The **entropy** measures the impurity or disorder of a dataset:\n",
    "$$\n",
    "H(S) = - \\sum_{i=1}^{c} p_i \\log_2 p_i\n",
    "$$\n",
    "where $S$ is the set of samples, $c$ is the number of classes, and $p_i$ is the proportion of samples belonging to class $i$.\n",
    "\n",
    "- **If all samples are of one class**: $H(S) = 0$.\n",
    "- **If samples are evenly distributed**: $H(S)$ is maximized.\n",
    "\n",
    "### Information Gain\n",
    "The **information gain** quantifies the reduction in entropy from a dataset $S$ after a split on attribute $A$:\n",
    "$$\n",
    "IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)\n",
    "$$\n",
    "where $S_v$ is the subset of $S$ where attribute $A$ has value $v$, and $|S|$ is the number of samples in $S$.\n",
    "\n",
    "###  Splitting Criteria\n",
    "For continuous attributes, the split is done based on a threshold $t$:\n",
    "- Split left: $A \\leq t$\n",
    "- Split right: $A > t$\n",
    "\n",
    "The goal is to choose the $A$ and $t$ that maximize $IG(S, A)$.\n",
    "\n",
    "###  Recursive Splitting Algorithm\n",
    "1. **Calculate entropy** of current set $S$.\n",
    "2. For each attribute $A$ and possible threshold $t$:\n",
    "   - Compute information gain $IG(S, A)$.\n",
    "3. **Select the best split** with maximum $IG(S, A)$.\n",
    "4. **Recursively split** the subsets $S_v$ until:\n",
    "   - Nodes are pure (all samples same class), or\n",
    "   - A stopping condition (max depth or min samples) is met.\n",
    "\n",
    "###  Majority Vote for Leaf Nodes\n",
    "When splitting cannot proceed:\n",
    "- **Assign the class** that has the majority in the node:\n",
    "$$\n",
    "Class = \\arg\\max_{i} (\\text{count of class } i)\n",
    "$$\n",
    "\n",
    "## Visual Intuition\n",
    "- **Entropy** visualizes the impurity of a node.\n",
    "- **Information gain** measures how much better a split makes the data.\n",
    "- The tree structure is built **top-down**, starting with the full dataset and splitting to maximize information gain.\n",
    "\n",
    "This theoretical foundation enables understanding the decision-making process of a decision tree before moving on to the code implementation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7128b783-faef-48b3-9e4a-525ef40326e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f144ec0-63bd-44fa-80c5-c8adc902f8c3",
   "metadata": {},
   "source": [
    "1- Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc7449c-75a2-42ed-95cf-2acf09fc9827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weather</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>PlayOutside</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sunny</td>\n",
       "      <td>hot</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>mild</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rainy</td>\n",
       "      <td>cool</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cloudy</td>\n",
       "      <td>cool</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cloudy</td>\n",
       "      <td>mild</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Weather Temperature PlayOutside\n",
       "0   sunny         hot          No\n",
       "1   sunny        mild         Yes\n",
       "2   rainy        cool          No\n",
       "3   rainy        mild          No\n",
       "4  cloudy        cool         Yes\n",
       "5  cloudy        mild         Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your dataset\n",
    "data = {\n",
    "    \"Weather\": [\"sunny\", \"sunny\", \"rainy\", \"rainy\", \"cloudy\", \"cloudy\"],\n",
    "    \"Temperature\": [\"hot\", \"mild\", \"cool\", \"mild\", \"cool\", \"mild\"],\n",
    "    \"PlayOutside\": [\"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"Yes\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e8c37a-5fa8-415d-be1f-7c7d6acbe158",
   "metadata": {},
   "source": [
    "2- Encode Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b588a1f-fe20-430d-8119-23d279b532da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weather</th>\n",
       "      <th>Weather_encoded</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Temperature_encoded</th>\n",
       "      <th>PlayOutside</th>\n",
       "      <th>Play_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sunny</td>\n",
       "      <td>2</td>\n",
       "      <td>hot</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>2</td>\n",
       "      <td>mild</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rainy</td>\n",
       "      <td>1</td>\n",
       "      <td>cool</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rainy</td>\n",
       "      <td>1</td>\n",
       "      <td>mild</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cloudy</td>\n",
       "      <td>0</td>\n",
       "      <td>cool</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cloudy</td>\n",
       "      <td>0</td>\n",
       "      <td>mild</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Weather  Weather_encoded Temperature  Temperature_encoded PlayOutside  \\\n",
       "0   sunny                2         hot                    1          No   \n",
       "1   sunny                2        mild                    2         Yes   \n",
       "2   rainy                1        cool                    0          No   \n",
       "3   rainy                1        mild                    2          No   \n",
       "4  cloudy                0        cool                    0         Yes   \n",
       "5  cloudy                0        mild                    2         Yes   \n",
       "\n",
       "   Play_encoded  \n",
       "0             0  \n",
       "1             1  \n",
       "2             0  \n",
       "3             0  \n",
       "4             1  \n",
       "5             1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual encoding for simplicity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_weather = LabelEncoder()\n",
    "le_temp = LabelEncoder()\n",
    "le_play = LabelEncoder()\n",
    "\n",
    "df[\"Weather_encoded\"] = le_weather.fit_transform(df[\"Weather\"])\n",
    "df[\"Temperature_encoded\"] = le_temp.fit_transform(df[\"Temperature\"])\n",
    "df[\"Play_encoded\"] = le_play.fit_transform(df[\"PlayOutside\"])\n",
    "\n",
    "df[[\"Weather\", \"Weather_encoded\", \"Temperature\", \"Temperature_encoded\", \"PlayOutside\", \"Play_encoded\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6b14d-360b-49c2-b4a2-ebcb6e5f3142",
   "metadata": {},
   "source": [
    "3.  Entropy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae6c84f-3f14-4427-8a34-e95c13fe633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    # Step 1: Compute the number of samples\n",
    "    # Variable: n\n",
    "    # Method: Use len(labels)\n",
    "    \n",
    "    # Step 2: Handle edge case when there are no samples\n",
    "    # Hint: Check if n == 0\n",
    "      # Entropy is 0.0 if dataset is empty\n",
    "    \n",
    "    # Step 3: Identify unique class labels\n",
    "    # Variable: unique_labels\n",
    "    # Method: Use np.unique(labels) to get the unique labels\n",
    "    \n",
    "    # Step 4: Compute probability for each unique label\n",
    "    # Variable: probs\n",
    "    # Hint: For each c in unique_labels, compute the proportion of samples with label c\n",
    "    # Method: (labels == c).sum() / n\n",
    "    \n",
    "    # Step 5: Calculate entropy\n",
    "    # Formula: entropy = -sum(p * log2(p)) for p > 0\n",
    "    # Method: Use generator expression with condition p > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848ccae1-fa8d-491c-bf78-723ad93d6575",
   "metadata": {},
   "source": [
    "4.  Information Gain Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dbe4003-4158-4fb9-992d-3dc37b8e77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X, y, feature_index, threshold):\n",
    "    # Step 1: Split data into left and right subsets based on threshold\n",
    "    # Variables: left_indices, right_indices\n",
    "    # Method: Use np.where to filter samples by threshold\n",
    "\n",
    "    \n",
    "    # Step 2: Get labels for left and right subsets\n",
    "    # Variables: left_y, right_y\n",
    "    # Method: Use y[left_indices] and y[right_indices]\n",
    " \n",
    "    \n",
    "    # Step 3: Compute entropy of the whole dataset before split\n",
    "    # Variable: entropy_before\n",
    "    # Method: Call entropy(y)\n",
    "    \n",
    "    # Step 4: Compute weighted entropy after split\n",
    "    # Variable: entropy_after\n",
    "    # Method: (len(left_y)/n)*entropy(left_y) + (len(right_y)/n)*entropy(right_y)\n",
    "\n",
    "    \n",
    "    # Step 5: Compute information gain\n",
    "    # Formula: gain = entropy_before - entropy_after\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a479bbcb-8304-4152-b66e-d5063fbd0466",
   "metadata": {},
   "source": [
    "5.  Calculate Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6682427-0bf2-4a80-9b59-8fc64e57b7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for Weather: 0.6667\n",
      "Information Gain for Temperature: 0.2075\n"
     ]
    }
   ],
   "source": [
    "gain_weather = info_gain(df, \"Weather_encoded\", \"Play_encoded\")\n",
    "gain_temp = info_gain(df, \"Temperature_encoded\", \"Play_encoded\")\n",
    "\n",
    "print(f\"Information Gain for Weather: {gain_weather:.4f}\")\n",
    "print(f\"Information Gain for Temperature: {gain_temp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2341c7-f0a4-41a2-a152-480a157d78c4",
   "metadata": {},
   "source": [
    "6.  Best split\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7cbe5c-d8e5-4dbc-838f-8d02c0191d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(X, y):\n",
    "    # Step 1: Initialize best variables\n",
    "    # Variables: best_gain, best_feature, best_threshold\n",
    "    # Hint: Set best_gain to -infinity initially\n",
    "    best_gain = -np.inf\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    \n",
    "    # Step 2: Get number of features in dataset\n",
    "    # Variable: n_features\n",
    "    # Method: X.shape[1]\n",
    "    \n",
    "    # Step 3: Loop over each feature\n",
    "    for feature_index in range(n_features):\n",
    "        # Step 4: Get unique threshold values for current feature\n",
    "        # Variable: thresholds\n",
    "        # Method: np.unique(X[:, feature_index])\n",
    "        \n",
    "        # Step 5: Try each threshold and compute information gain\n",
    "        for threshold in thresholds:\n",
    "            # Variable: gain\n",
    "            # Method: Call information_gain(X, y, feature_index, threshold)\n",
    "            \n",
    "            # Step 6: Update best split if gain is higher\n",
    "            if gain > best_gain:\n",
    "              \n",
    "    # Step 7: Return best split information\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a1b75-ea18-4bf5-82ec-bad2fa0b77e3",
   "metadata": {},
   "source": [
    "8- Majority class calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb052e8e-fb40-4d9f-b231-ebf1ead6d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_class(labels):\n",
    "    # Step 1: Find unique class labels and their counts\n",
    "    # Variables: unique, counts\n",
    "    # Method: np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Step 2: Find the index of the maximum count (most frequent label)\n",
    "    # Variable: max_index\n",
    "    # Method: np.argmax(counts)\n",
    "    \n",
    "    # Step 3: Return the label corresponding to max_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608ce27-2322-4884-ae58-0c651c3fd2db",
   "metadata": {},
   "source": [
    "8-Build tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ddafac-8662-4e88-a3a2-0799df5032c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X, y, depth=0, max_depth=3):\n",
    "    # Step 1: Define stopping conditions\n",
    "    # If entropy of current labels is zero OR if only one sample OR if maximum depth reached\n",
    "    # Variables: entropy(y), len(y), depth\n",
    "     # Leaf node with class label\n",
    "    \n",
    "    # Step 2: Find the best split (feature and threshold)\n",
    "    # Variables: feature_index, threshold, gain\n",
    "    # Method: best_split(X, y)\n",
    "    \n",
    "    # Step 3: If no information gain, return majority class as leaf\n",
    "\n",
    "    \n",
    "    # Step 4: Partition the data into left and right subsets\n",
    "    # Variables: left_indices, right_indices\n",
    "    # Method: np.where(X[:, feature_index] <= threshold) and > threshold\n",
    "  \n",
    "    \n",
    "    # Step 5: Recursively build left and right subtrees\n",
    "    # Variables: left_subtree, right_subtree\n",
    "    # Method: build_tree with depth+1\n",
    "    \n",
    "    \n",
    "    # Step 6: Return the current node as a dictionary with split information\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac0caf2-ce28-4b55-81f7-632273ab20a0",
   "metadata": {},
   "source": [
    "9-Predict one and multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52b294b-34b6-4cd0-a90d-7b1515f92c05",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1942456704.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def predict_one(x, tree):\n",
    "    # Step 1: Check if current tree node is a leaf node\n",
    "    # Condition: if tree is not a dictionary (i.e., a class label)\n",
    "    # Return the class label\n",
    "    \n",
    "    # Step 2: Extract feature index and threshold from current node\n",
    "    \n",
    "    # Step 3: Check if sample's feature value is less than or equal to threshold\n",
    "    if x[feature_index] <= threshold:\n",
    "        # Recursively predict using left subtree\n",
    "    else:\n",
    "        # Recursively predict using right subtree\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5501b-62f6-4dfa-8771-684f57f844aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, tree):\n",
    "    # Step 1: For each sample x in dataset X, predict the class label\n",
    "    # Method: list comprehension with predict_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21cd89a5-590f-45a1-b9fd-448e20bea9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree structure:\n",
      "{'feature_index': 0, 'threshold': 2, 'left': {'feature_index': 0, 'threshold': 1, 'left': {'feature_index': 1, 'threshold': 1, 'left': 0, 'right': 1}, 'right': 0}, 'right': 1}\n",
      "Predictions: [0 1 0 0 1 1]\n",
      "True labels: [0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(labels):\n",
    "    n = len(labels)\n",
    "    if n == 0:\n",
    "        return 0.0  # entropy is zero if no samples\n",
    "    unique_labels = np.unique(labels)\n",
    "    probs = [(labels == c).sum() / n for c in unique_labels]\n",
    "    return -sum(p * np.log2(p) for p in probs if p > 0)\n",
    "\n",
    "def information_gain(X, y, feature_index, threshold):\n",
    "    # split data by threshold on feature\n",
    "    left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "    right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "\n",
    "    left_y = y[left_indices]\n",
    "    right_y = y[right_indices]\n",
    "\n",
    "    entropy_before = entropy(y)\n",
    "    n = len(y)\n",
    "    entropy_after = (len(left_y) / n) * entropy(left_y) + (len(right_y) / n) * entropy(right_y)\n",
    "    gain = entropy_before - entropy_after\n",
    "    return gain\n",
    "\n",
    "def best_split(X, y):\n",
    "    best_gain = -np.inf\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    for feature_index in range(n_features):\n",
    "        thresholds = np.unique(X[:, feature_index])\n",
    "        for threshold in thresholds:\n",
    "            gain = information_gain(X, y, feature_index, threshold)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature_index\n",
    "                best_threshold = threshold\n",
    "    return best_feature, best_threshold, best_gain\n",
    "\n",
    "def majority_class(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    return unique[np.argmax(counts)]\n",
    "\n",
    "def build_tree(X, y, depth=0, max_depth=3):\n",
    "    # Stopping conditions\n",
    "    if entropy(y) == 0 or len(y) <= 1 or depth >= max_depth:\n",
    "        return majority_class(y)  # leaf node with class label\n",
    "    \n",
    "    feature_index, threshold, gain = best_split(X, y)\n",
    "    \n",
    "    if gain == 0:\n",
    "        return majority_class(y)  # no info gain, make leaf\n",
    "    \n",
    "    left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "    right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "\n",
    "    left_subtree = build_tree(X[left_indices], y[left_indices], depth + 1, max_depth)\n",
    "    right_subtree = build_tree(X[right_indices], y[right_indices], depth + 1, max_depth)\n",
    "\n",
    "    # Return the tree as a dictionary (node)\n",
    "    return {\n",
    "        'feature_index': feature_index,\n",
    "        'threshold': threshold,\n",
    "        'left': left_subtree,\n",
    "        'right': right_subtree\n",
    "    }\n",
    "\n",
    "def predict_one(x, tree):\n",
    "    # If leaf node, tree is a class label (int or str)\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    \n",
    "    feature_index = tree['feature_index']\n",
    "    threshold = tree['threshold']\n",
    "    \n",
    "    if x[feature_index] <= threshold:\n",
    "        return predict_one(x, tree['left'])\n",
    "    else:\n",
    "        return predict_one(x, tree['right'])\n",
    "\n",
    "def predict(X, tree):\n",
    "    return np.array([predict_one(x, tree) for x in X])\n",
    "\n",
    "# Example usage:\n",
    "X = np.array([\n",
    "    [1, 1],\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [2, 2],\n",
    "    [3, 3],\n",
    "    [3, 2]\n",
    "])\n",
    "y = np.array([0, 1, 0, 0, 1, 1])\n",
    "\n",
    "tree = build_tree(X, y, max_depth=3)\n",
    "print(\"Decision tree structure:\")\n",
    "print(tree)\n",
    "\n",
    "# Predict on training data\n",
    "predictions = predict(X, tree)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"True labels:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3adddc9-dad6-4f55-a5e4-00e1dcbd39ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if Feature 0 <= 2:\n",
      "  if Feature 0 <= 1:\n",
      "    if Feature 1 <= 1:\n",
      "      Leaf: 0\n",
      "    else:  # Feature 1 > 1\n",
      "      Leaf: 1\n",
      "  else:  # Feature 0 > 1\n",
      "    Leaf: 0\n",
      "else:  # Feature 0 > 2\n",
      "  Leaf: 1\n"
     ]
    }
   ],
   "source": [
    "def print_tree(tree, feature_names=None, indent=\"\"):\n",
    "    if not isinstance(tree, dict):\n",
    "        print(indent + \"Leaf:\", tree)\n",
    "        return\n",
    "    \n",
    "    feature = feature_names[tree['feature_index']] if feature_names else f\"X[{tree['feature_index']}]\"\n",
    "    threshold = tree['threshold']\n",
    "    print(f\"{indent}if {feature} <= {threshold}:\")\n",
    "    print_tree(tree['left'], feature_names, indent + \"  \")\n",
    "    print(f\"{indent}else:  # {feature} > {threshold}\")\n",
    "    print_tree(tree['right'], feature_names, indent + \"  \")\n",
    "\n",
    "# Example usage with feature names:\n",
    "tree = {'feature_index': 0, 'threshold': 2,\n",
    "        'left': {'feature_index': 0, 'threshold': 1,\n",
    "                 'left': {'feature_index': 1, 'threshold': 1, 'left': 0, 'right': 1},\n",
    "                 'right': 0},\n",
    "        'right': 1}\n",
    "\n",
    "print_tree(tree, feature_names=['Feature 0', 'Feature 1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5538e-fcda-4427-9e69-ea96f141642f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Decision Trees are intuitive and interpretable models that recursively split data based on feature values to create a tree structure representing decision rules. They are easy to visualize and understand, making them valuable for exploratory data analysis and baseline models.\n",
    "\n",
    "Key points:\n",
    "- **Splitting criteria** like Gini impurity or entropy guide the construction of the tree to maximize information gain.\n",
    "- Trees can handle both numerical and categorical data without requiring extensive preprocessing.\n",
    "- **Overfitting** is a common challenge, often mitigated by limiting tree depth, minimum samples per leaf, or pruning.\n",
    "- Decision Trees form the foundation for more advanced ensemble methods like Random Forests and Gradient Boosting.\n",
    "\n",
    "Mastering Decision Trees provides a strong base for understanding complex machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a32d4-2f9f-41d9-961f-5ea7a7410e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
