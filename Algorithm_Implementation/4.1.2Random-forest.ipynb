{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90ecb18",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left;\">\n",
    "  <a href=\"https://code213.tech/\" target=\"_blank\">\n",
    "    <img src=\"../images/code213.PNG\" alt=\"QWorld\">\n",
    "  </a>\n",
    "  <p><em>prepared by Latreche Sara</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33956c84-0e74-4219-8931-750120e5cb9e",
   "metadata": {},
   "source": [
    "# Random Forest From Scratch: A Beginner-Friendly Guide\n",
    "\n",
    "In this notebook, we will explore the **Random Forest** algorithm, a powerful ensemble learning method used for classification and regression tasks.\n",
    "\n",
    "We will cover:\n",
    "- The intuition behind Random Forests\n",
    "- Important concepts such as decision trees, impurity measures (Gini impurity)\n",
    "- How trees are built and combined\n",
    "- Step-by-step mathematical definitions\n",
    "- A blueprint for implementing Random Forest from scratch\n",
    "\n",
    "This notebook contains **only theoretical explanations and formulas** in markdown cells — a complete conceptual guide before coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b22f20a-243f-451a-8623-eaafa5c3e42b",
   "metadata": {},
   "source": [
    "## What is a Decision Tree?\n",
    "\n",
    "A decision tree is a flowchart-like tree structure where:\n",
    "\n",
    "- Each internal node represents a **decision** based on a feature and threshold.\n",
    "- Each branch represents the outcome of the decision.\n",
    "- Each leaf node represents a **class label** (for classification) or a value (for regression).\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "A decision tree partitions the data step-by-step, trying to group samples with the same label together, so that the final leaves are as \"pure\" as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31ec4b-82fc-48b9-9569-a72898df7bf7",
   "metadata": {},
   "source": [
    "## Measuring Node Purity: Gini Impurity\n",
    "\n",
    "To build a good tree, we need to decide **where to split** the data. This requires a measure of how \"impure\" a node is.\n",
    "\n",
    "The **Gini impurity** quantifies this impurity for classification problems.\n",
    "\n",
    "### Definition:\n",
    "\n",
    "$$\n",
    "Gini(y) = 1 - \\sum_{i=1}^C p_i^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $(D\\)$ is the current dataset (node),\n",
    "- $\\(k\\)$ is the number of classes,\n",
    "- $\\(p_i\\) $is the proportion of class \\(i\\) samples in \\(D\\).\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- $\\(Gini = 0\\)$ means the node is pure (all samples same class).\n",
    "- The higher the Gini, the more mixed the classes are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c35eaed-d5d1-4cad-ad9f-82a6edb90cd4",
   "metadata": {},
   "source": [
    "## Dataset Splitting and Weighted Gini\n",
    "\n",
    "When we split a node, it divides data into two subsets:\n",
    "\n",
    "- $D_{\\text{left}}$: samples where feature $\\leq$ threshold  \n",
    "- $D_{\\text{right}}$: samples where feature $>$ threshold\n",
    "\n",
    "We want to find the split that **minimizes the weighted average impurity** of the two subsets:\n",
    "\n",
    "$$\n",
    "Gini_{\\text{split}} = \\frac{|D_{\\text{left}}|}{|D|} \\cdot Gini(D_{\\text{left}}) + \\frac{|D_{\\text{right}}|}{|D|} \\cdot Gini(D_{\\text{right}})\n",
    "$$\n",
    "\n",
    "The goal is to find the **feature and threshold** that yields the lowest $Gini_{\\text{split}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c111b0-7045-4221-9e0b-b8774805b33b",
   "metadata": {},
   "source": [
    "## Building a Decision Tree — When to Stop?\n",
    "\n",
    "During tree construction, we stop splitting when:\n",
    "\n",
    "- All samples in a node belong to the same class (pure node),\n",
    "- Maximum tree depth is reached,\n",
    "- The node contains fewer than a minimum number of samples,\n",
    "- Or when no split improves impurity.\n",
    "\n",
    "At these points, the node becomes a **leaf node**, assigned the most common class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d239280-c9b9-453f-bbf5-caec47ff8d72",
   "metadata": {},
   "source": [
    "## Intuition Behind Random Forest\n",
    "\n",
    "Random Forest combines many decision trees to improve prediction and reduce overfitting.\n",
    "\n",
    "Key ideas:\n",
    "- **Bootstrap sampling**: Each tree is trained on a random subset (with replacement) of the data.\n",
    "- **Random feature selection**: At each split, only a random subset of features is considered.\n",
    "- This randomness **decorrelates** trees, making the ensemble more robust.\n",
    "\n",
    "The final prediction is obtained by **aggregating** the predictions of all trees, usually by majority vote (classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d30762-7bd6-45f0-af42-75dd71b1436c",
   "metadata": {},
   "source": [
    "## Random Forest Algorithm — Step-by-Step\n",
    "\n",
    "Given training data $(X, y)$:\n",
    "\n",
    "1. For $t = 1, \\ldots, T$ (number of trees):\n",
    "    - Draw a **bootstrap sample** $D_t$ by sampling $N$ examples from $(X, y)$ **with replacement**.\n",
    "    - Build a decision tree $h_t$ using $D_t$, where:\n",
    "        - At each split, select a random subset of $m$ features.\n",
    "        - Choose the best split among these features to minimize impurity (Gini).\n",
    "        - Grow the tree fully or until stopping criteria are met.\n",
    "2. To predict a new sample $x$, aggregate predictions from all trees:\n",
    "    $$\n",
    "    \\hat{y} = \\text{majority vote} \\big( h_1(x), h_2(x), \\ldots, h_T(x) \\big)\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3de9f-8e3b-4a3d-9d3a-9f83dbc83da2",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "- Decision Trees split data to form pure leaves using impurity measures like Gini impurity.\n",
    "- Random Forest builds an ensemble of decision trees trained on random subsets of data and features.\n",
    "- Aggregating many diverse trees improves generalization and reduces overfitting.\n",
    "\n",
    "In the next notebook cells, we will implement:\n",
    "\n",
    "- Gini impurity calculation,\n",
    "- Dataset splitting,\n",
    "- Decision tree building,\n",
    "- Random forest ensemble training,\n",
    "- And prediction functions.\n",
    "\n",
    "This conceptual foundation will make coding these algorithms easier to understand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cd127-168f-456e-94ba-49baabe21f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gini impurity function\n",
    "\n",
    "def gini_impurity(y):\n",
    "    # Step 1: Check if the list y is empty\n",
    "   \n",
    "    \n",
    "    # Step 2: Calculate the class probabilities using Counter\n",
    "    # Method: Use Counter(y).values() to get counts of each class\n",
    "    # Convert counts to probabilities by dividing by total samples len(y)\n",
    "    \n",
    "    # Step 3: Calculate Gini impurity: 1 - sum of squared probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25068dfc-1f07-461d-937b-48fb5b8cb55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset splitting function\n",
    "def split_dataset(X, y, feature, threshold):\n",
    "    # Step 1: Create a boolean mask where feature values <= threshold (left split)\n",
    "    \n",
    "    # Step 2: Create complement mask for right split\n",
    "    \n",
    "    # Step 3: Use masks to index X and y, returning left and right splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f6842-295f-4898-a036-d525b5569634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(X, y, feature_subset):\n",
    "    # Step 1: Initialize best split variables\n",
    "    # Variables: best_feature, best_threshold, best_gini, best_split\n",
    "    best_feature, best_threshold, best_gini, best_split = None, None, float('inf'), None\n",
    "    \n",
    "    # Step 2: Loop over each feature in the subset\n",
    "    # Loop variable: feature\n",
    "    for feature in feature_subset:\n",
    "        # Step 3: Find unique threshold candidates from feature values\n",
    "        # Method: np.unique, Variable: thresholds\n",
    "        \n",
    "        # Step 4: For each threshold, split dataset and compute weighted Gini impurity\n",
    "        # Loop variable: threshold\n",
    "        for threshold in thresholds:\n",
    "            # Method: split_dataset, Variables: X_left, y_left, X_right, y_right\n",
    "            \n",
    "            # Step 5: Skip if one side is empty (invalid split)\n",
    "            # Variables: y_left, y_right (length check)\n",
    "            \n",
    "            \n",
    "            # Step 6: Calculate weighted average Gini impurity for this split\n",
    "            # Method: gini_impurity, Variable: gini\n",
    "            \n",
    "            # Step 7: Update best split if current Gini is lower\n",
    "            # Variables updated: best_gini, best_feature, best_threshold, best_split\n",
    "            if gini < best_gini:\n",
    "               \n",
    "    \n",
    "    # Step 8: Return best feature, threshold, and split subsets\n",
    "    # Return: best_feature, best_threshold, best_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a47b8-0e33-49fb-9ee6-0c3298e034f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386aa4e3-248c-410e-8017-ab2ee67add70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees in the forest: 10\n",
      "Tree 1:\n",
      "Feature 0 <= 0.6480914957655985\n",
      "  Feature 0 <= -0.5637579631780603\n",
      "    Feature 0 <= -0.9631613385037517\n",
      "      Predict: 1\n",
      "    Feature 0 > -0.9631613385037517\n",
      "      Feature 1 <= -0.011701951630812002\n",
      "        Predict: 1\n",
      "      Feature 1 > -0.011701951630812002\n",
      "        Predict: 0\n",
      "  Feature 0 > -0.5637579631780603\n",
      "    Feature 0 <= 0.6187306455587391\n",
      "      Feature 0 <= 0.5050595390095056\n",
      "        Feature 0 <= -0.24379204862887738\n",
      "          Predict: 0\n",
      "        Feature 0 > -0.24379204862887738\n",
      "          Predict: 1\n",
      "      Feature 0 > 0.5050595390095056\n",
      "        Predict: 0\n",
      "    Feature 0 > 0.6187306455587391\n",
      "      Predict: 1\n",
      "Feature 0 > 0.6480914957655985\n",
      "  Feature 0 <= 1.4798734979244375\n",
      "    Predict: 0\n",
      "  Feature 0 > 1.4798734979244375\n",
      "    Feature 0 <= 1.484900192274098\n",
      "      Predict: 1\n",
      "    Feature 0 > 1.484900192274098\n",
      "      Predict: 0\n",
      "\n",
      "\n",
      "Tree 2:\n",
      "Feature 1 <= -0.15647560181783404\n",
      "  Predict: 1\n",
      "Feature 1 > -0.15647560181783404\n",
      "  Feature 1 <= 0.674808475189397\n",
      "    Feature 1 <= 0.665947973754857\n",
      "      Feature 0 <= 1.484900192274098\n",
      "        Feature 0 <= 1.3135391018405107\n",
      "          Predict: 0\n",
      "        Feature 0 > 1.3135391018405107\n",
      "          Predict: 1\n",
      "      Feature 0 > 1.484900192274098\n",
      "        Predict: 0\n",
      "    Feature 1 > 0.665947973754857\n",
      "      Predict: 1\n",
      "  Feature 1 > 0.674808475189397\n",
      "    Feature 1 <= 1.2500708774978606\n",
      "      Predict: 0\n",
      "    Feature 1 > 1.2500708774978606\n",
      "      Feature 0 <= -1.8250309686619484\n",
      "        Predict: 1\n",
      "      Feature 0 > -1.8250309686619484\n",
      "        Predict: 0\n",
      "\n",
      "\n",
      "Tree 3:\n",
      "Feature 0 <= 0.6480914957655985\n",
      "  Feature 0 <= -0.9631613385037517\n",
      "    Predict: 1\n",
      "  Feature 0 > -0.9631613385037517\n",
      "    Feature 1 <= -0.11491107925688648\n",
      "      Predict: 1\n",
      "    Feature 1 > -0.11491107925688648\n",
      "      Feature 0 <= -0.5637579631780603\n",
      "        Feature 1 <= -0.011701951630812002\n",
      "          Predict: 1\n",
      "        Feature 1 > -0.011701951630812002\n",
      "          Predict: 0\n",
      "      Feature 0 > -0.5637579631780603\n",
      "        Feature 1 <= 0.218247648585304\n",
      "          Predict: 0\n",
      "        Feature 1 > 0.218247648585304\n",
      "          Predict: 0\n",
      "Feature 0 > 0.6480914957655985\n",
      "  Feature 0 <= 1.4798734979244375\n",
      "    Predict: 0\n",
      "  Feature 0 > 1.4798734979244375\n",
      "    Feature 1 <= -1.2746492632344077\n",
      "      Predict: 1\n",
      "    Feature 1 > -1.2746492632344077\n",
      "      Predict: 0\n",
      "\n",
      "\n",
      "Tree 4:\n",
      "Feature 0 <= 0.5652003464600557\n",
      "  Feature 0 <= -0.32215389435407715\n",
      "    Feature 0 <= -0.9631613385037517\n",
      "      Predict: 1\n",
      "    Feature 0 > -0.9631613385037517\n",
      "      Feature 1 <= -0.011701951630812002\n",
      "        Feature 0 <= -0.5637579631780603\n",
      "          Predict: 1\n",
      "        Feature 0 > -0.5637579631780603\n",
      "          Predict: 1\n",
      "      Feature 1 > -0.011701951630812002\n",
      "        Predict: 0\n",
      "  Feature 0 > -0.32215389435407715\n",
      "    Feature 1 <= -0.11491107925688648\n",
      "      Predict: 1\n",
      "    Feature 1 > -0.11491107925688648\n",
      "      Feature 0 <= 0.0049551826015605815\n",
      "        Feature 1 <= 0.218247648585304\n",
      "          Predict: 1\n",
      "        Feature 1 > 0.218247648585304\n",
      "          Predict: 0\n",
      "      Feature 0 > 0.0049551826015605815\n",
      "        Predict: 0\n",
      "Feature 0 > 0.5652003464600557\n",
      "  Feature 1 <= -1.2588381460829534\n",
      "    Predict: 1\n",
      "  Feature 1 > -1.2588381460829534\n",
      "    Predict: 0\n",
      "\n",
      "\n",
      "Tree 5:\n",
      "Feature 1 <= 0.4735261427008537\n",
      "  Feature 0 <= 1.549212218937435\n",
      "    Feature 0 <= 0.6480914957655985\n",
      "      Predict: 1\n",
      "    Feature 0 > 0.6480914957655985\n",
      "      Feature 0 <= 1.1127458818127236\n",
      "        Predict: 0\n",
      "      Feature 0 > 1.1127458818127236\n",
      "        Predict: 1\n",
      "  Feature 0 > 1.549212218937435\n",
      "    Predict: 0\n",
      "Feature 1 > 0.4735261427008537\n",
      "  Feature 1 <= 0.674808475189397\n",
      "    Feature 1 <= 0.665947973754857\n",
      "      Predict: 0\n",
      "    Feature 1 > 0.665947973754857\n",
      "      Predict: 1\n",
      "  Feature 1 > 0.674808475189397\n",
      "    Feature 1 <= 1.2500708774978606\n",
      "      Predict: 0\n",
      "    Feature 1 > 1.2500708774978606\n",
      "      Feature 1 <= 1.2931965740781504\n",
      "        Predict: 1\n",
      "      Feature 1 > 1.2931965740781504\n",
      "        Predict: 0\n",
      "\n",
      "\n",
      "Tree 6:\n",
      "Feature 0 <= 0.06588671492265408\n",
      "  Feature 0 <= -0.899386731845852\n",
      "    Predict: 1\n",
      "  Feature 0 > -0.899386731845852\n",
      "    Feature 1 <= -0.7257330214329063\n",
      "      Predict: 1\n",
      "    Feature 1 > -0.7257330214329063\n",
      "      Feature 1 <= -0.07067783072861689\n",
      "        Predict: 0\n",
      "      Feature 1 > -0.07067783072861689\n",
      "        Feature 1 <= 0.218247648585304\n",
      "          Predict: 1\n",
      "        Feature 1 > 0.218247648585304\n",
      "          Predict: 0\n",
      "Feature 0 > 0.06588671492265408\n",
      "  Feature 1 <= -0.11491107925688648\n",
      "    Predict: 1\n",
      "  Feature 1 > -0.11491107925688648\n",
      "    Predict: 0\n",
      "\n",
      "\n",
      "Tree 7:\n",
      "Feature 0 <= 0.6480914957655985\n",
      "  Feature 1 <= 0.674808475189397\n",
      "    Feature 1 <= -0.011701951630812002\n",
      "      Predict: 1\n",
      "    Feature 1 > -0.011701951630812002\n",
      "      Feature 1 <= 0.00226760699807238\n",
      "        Predict: 0\n",
      "      Feature 1 > 0.00226760699807238\n",
      "        Feature 1 <= 0.218247648585304\n",
      "          Predict: 1\n",
      "        Feature 1 > 0.218247648585304\n",
      "          Predict: 1\n",
      "  Feature 1 > 0.674808475189397\n",
      "    Feature 1 <= 1.2931965740781504\n",
      "      Feature 0 <= -1.8250309686619484\n",
      "        Predict: 1\n",
      "      Feature 0 > -1.8250309686619484\n",
      "        Predict: 0\n",
      "    Feature 1 > 1.2931965740781504\n",
      "      Predict: 0\n",
      "Feature 0 > 0.6480914957655985\n",
      "  Feature 0 <= 0.9779341575264029\n",
      "    Feature 0 <= 0.8195430494823092\n",
      "      Predict: 0\n",
      "    Feature 0 > 0.8195430494823092\n",
      "      Predict: 1\n",
      "  Feature 0 > 0.9779341575264029\n",
      "    Predict: 0\n",
      "\n",
      "\n",
      "Tree 8:\n",
      "Feature 0 <= 0.6480914957655985\n",
      "  Feature 1 <= -0.15647560181783404\n",
      "    Predict: 1\n",
      "  Feature 1 > -0.15647560181783404\n",
      "    Feature 1 <= 0.4319311037846052\n",
      "      Feature 1 <= -0.07067783072861689\n",
      "        Predict: 0\n",
      "      Feature 1 > -0.07067783072861689\n",
      "        Predict: 1\n",
      "    Feature 1 > 0.4319311037846052\n",
      "      Predict: 0\n",
      "Feature 0 > 0.6480914957655985\n",
      "  Feature 1 <= 0.4735261427008537\n",
      "    Feature 1 <= 0.46008614864025477\n",
      "      Predict: 0\n",
      "    Feature 1 > 0.46008614864025477\n",
      "      Predict: 1\n",
      "  Feature 1 > 0.4735261427008537\n",
      "    Predict: 0\n",
      "\n",
      "\n",
      "Tree 9:\n",
      "Feature 0 <= 0.5652003464600557\n",
      "  Feature 1 <= 0.674808475189397\n",
      "    Feature 1 <= -0.11491107925688648\n",
      "      Predict: 1\n",
      "    Feature 1 > -0.11491107925688648\n",
      "      Feature 0 <= -0.9631613385037517\n",
      "        Feature 1 <= 0.665947973754857\n",
      "          Predict: 1\n",
      "        Feature 1 > 0.665947973754857\n",
      "          Predict: 1\n",
      "      Feature 0 > -0.9631613385037517\n",
      "        Feature 1 <= 0.00226760699807238\n",
      "          Predict: 0\n",
      "        Feature 1 > 0.00226760699807238\n",
      "          Predict: 1\n",
      "  Feature 1 > 0.674808475189397\n",
      "    Predict: 0\n",
      "Feature 0 > 0.5652003464600557\n",
      "  Feature 0 <= 1.549212218937435\n",
      "    Feature 0 <= 1.4017556515388065\n",
      "      Feature 0 <= 0.9208922048738221\n",
      "        Predict: 0\n",
      "      Feature 0 > 0.9208922048738221\n",
      "        Feature 0 <= 0.9779341575264029\n",
      "          Predict: 1\n",
      "        Feature 0 > 0.9779341575264029\n",
      "          Predict: 0\n",
      "    Feature 0 > 1.4017556515388065\n",
      "      Predict: 1\n",
      "  Feature 0 > 1.549212218937435\n",
      "    Predict: 0\n",
      "\n",
      "\n",
      "Tree 10:\n",
      "Feature 1 <= -0.18281809678166272\n",
      "  Predict: 1\n",
      "Feature 1 > -0.18281809678166272\n",
      "  Feature 0 <= -1.5750220891148499\n",
      "    Predict: 1\n",
      "  Feature 0 > -1.5750220891148499\n",
      "    Feature 0 <= 0.3581419564131616\n",
      "      Feature 0 <= 0.3115401962594021\n",
      "        Feature 1 <= 0.218247648585304\n",
      "          Predict: 1\n",
      "        Feature 1 > 0.218247648585304\n",
      "          Predict: 0\n",
      "      Feature 0 > 0.3115401962594021\n",
      "        Predict: 1\n",
      "    Feature 0 > 0.3581419564131616\n",
      "      Predict: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Gini impurity function\n",
    "def gini_impurity(y):\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    probs = np.array(list(Counter(y).values())) / len(y)\n",
    "    return 1 - np.sum(probs ** 2)\n",
    "\n",
    "# Function to split dataset based on feature and threshold\n",
    "def split_dataset(X, y, feature, threshold):\n",
    "    left_idx = X[:, feature] <= threshold\n",
    "    right_idx = ~left_idx\n",
    "    return X[left_idx], y[left_idx], X[right_idx], y[right_idx]\n",
    "\n",
    "# Function to find the best split\n",
    "def best_split(X, y, feature_subset):\n",
    "    best_feature, best_threshold, best_gini, best_split = None, None, float('inf'), None\n",
    "    for feature in feature_subset:\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            X_left, y_left, X_right, y_right = split_dataset(X, y, feature, threshold)\n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "            gini = (len(y_left) * gini_impurity(y_left) + len(y_right) * gini_impurity(y_right)) / len(y)\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                best_split = (X_left, y_left, X_right, y_right)\n",
    "    return best_feature, best_threshold, best_split\n",
    "\n",
    "# Tree node class\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, prediction=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.prediction = prediction\n",
    "\n",
    "# Function to build the tree\n",
    "def build_tree(X, y, depth=0, max_depth=5, min_samples=2):\n",
    "    if len(y) <= min_samples or depth >= max_depth or gini_impurity(y) == 0:\n",
    "        prediction = Counter(y).most_common(1)[0][0]\n",
    "        return TreeNode(prediction=prediction)\n",
    "\n",
    "    feature_subset = random.sample(range(X.shape[1]), int(np.sqrt(X.shape[1])) or 1)\n",
    "    feature, threshold, split = best_split(X, y, feature_subset)\n",
    "\n",
    "    if feature is None:\n",
    "        prediction = Counter(y).most_common(1)[0][0]\n",
    "        return TreeNode(prediction=prediction)\n",
    "\n",
    "    X_left, y_left, X_right, y_right = split\n",
    "    left_node = build_tree(X_left, y_left, depth + 1, max_depth, min_samples)\n",
    "    right_node = build_tree(X_right, y_right, depth + 1, max_depth, min_samples)\n",
    "    return TreeNode(feature, threshold, left_node, right_node)\n",
    "\n",
    "# Prediction function for a tree\n",
    "def predict_tree(node, x):\n",
    "    if node.feature is None:\n",
    "        return node.prediction\n",
    "    if x[node.feature] <= node.threshold:\n",
    "        return predict_tree(node.left, x)\n",
    "    else:\n",
    "        return predict_tree(node.right, x)\n",
    "\n",
    "# Random forest class\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=5, min_samples=2):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        for _ in range(self.n_trees):\n",
    "            idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_sample, y_sample = X[idxs], y[idxs]\n",
    "            tree = build_tree(X_sample, y_sample, 0, self.max_depth, self.min_samples)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        all_preds = np.array([[predict_tree(tree, x) for x in X] for tree in self.trees])\n",
    "        return [Counter(all_preds[:, i]).most_common(1)[0][0] for i in range(X.shape[0])]\n",
    "\n",
    "    def print_trees(self):\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            print(f\"Tree {i+1}:\")\n",
    "            print_tree(tree)\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Helper function to print the tree\n",
    "\n",
    "def print_tree(node, indent=\"\"):\n",
    "    if node.feature is None:\n",
    "        print(indent + f\"Predict: {node.prediction}\")\n",
    "    else:\n",
    "        print(indent + f\"Feature {node.feature} <= {node.threshold}\")\n",
    "        print_tree(node.left, indent + \"  \")\n",
    "        print(indent + f\"Feature {node.feature} > {node.threshold}\")\n",
    "        print_tree(node.right, indent + \"  \")\n",
    "\n",
    "# Example usage\n",
    "X_train = np.vstack([\n",
    "    np.column_stack((np.random.randn(50) + 1, np.random.randn(50) + 1)),\n",
    "    np.column_stack((np.random.randn(50) - 1, np.random.randn(50) - 1))\n",
    "])\n",
    "y_train = np.array([0]*50 + [1]*50)\n",
    "\n",
    "rf = RandomForest(n_trees=10, max_depth=5, min_samples=2)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Number of trees in the forest: {len(rf.trees)}\")\n",
    "rf.print_trees()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc97a74-c68a-48a5-a17a-0c002f3b10ba",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Random Forest is a powerful ensemble learning method that combines multiple decision trees to improve classification or regression performance. By using bootstrap sampling and random feature selection at each split, Random Forest reduces overfitting and increases model robustness. \n",
    "\n",
    "Key takeaways:\n",
    "- **Ensemble of trees:** Combines many weak learners into a strong one.\n",
    "- **Randomness:** Injected via bootstrap samples and feature subsets, enhancing generalization.\n",
    "- **Gini impurity:** Measures node impurity and guides splits for better class separation.\n",
    "- **Majority voting:** Aggregates predictions from all trees for final output.\n",
    "\n",
    "Understanding these fundamentals enables you to implement and tune Random Forest models effectively from scratch.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
